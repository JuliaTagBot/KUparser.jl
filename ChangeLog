2015-07-27  Deniz Yuret  <dyuret@ku.edu.tr>

	* profile: features.pl not having ldep/rdep adds 10% to time.  on
	the other hand it will speed up alloc and copy of parsers and
	memory savings.  need experiment to decide.  alternative double
	array with push.  other than that converting features to
	ASCIIString instead of String, writing to preallocated feature
	matrix, not doing gc() every call to predict more than doubled the
	speed.  bparser working correctly except (1) return xy (2)
	multi-cpu version.

	* yatbazp-static.out: TODO: static/dynamic experiments with
	yatbazp failed.  Parser never finished first epoch.

2015-07-26  Deniz Yuret  <dyuret@ku.edu.tr>

	* bparser.jl: figured out the math: update with 1-p for oracle
	moves, -p for all other moves where p is the path probability.  we
	take the paths on beam at the time of early stop to be the whole
	set of paths as an approx.  when the top p approx 1, this is the
	same as the perceptron update: if the top p is the oracle path, we
	get no update, if it is another path we get +1 for oracle path and
	-1 for the maxscore path.  this also means we need to keep the
	history of moves for each path.  To figure out how good the
	perceptron approx is we can use an existing model with beam
	decoding and see the difference between the top 2 paths.  To
	figure out how good the beam approx is, we could increase beam
	size?  (still not going to tell us for sure).  Ideas:

	;;; replicate zn11 with sparse features and struct perceptron.
	;;; instrument bparser to get statistics on path scores.
	;;; debug the new nnet update rule on a simpler tagging problem.
	;;; apply the new update rule in bparser starting with a good model.

	;;; fix bparser to keep the whole history.
	;; we need to keep around all the x's
	;; we need to calculate cumulative updates for common ancestors
	; allocate one big x matrix for the whole sentence?
	; we already have fidx in the parser state.
	; use a resizeable x?
	;; we will return a small subset back for training?
	; a fixed number (two per move) if doing perceptron updates.
	; however during the time when two paths together none.
	; a larger subset if regular updates (all except childless ancestors)
	; or we can use childless ancestors as well?
	; no,their final scores are presumably too small
	;; if we still group x's for efficient prediction we need to keep
	pointers
	;; we need to keep around all the parser states

	;;; will this fit in memory?
	; for a 40 word sentence:
	; we have 2n-2=78 moves.
	; beam=64 => 78x64=4992 parser states on the beam.
	; we have 3105 feat dims => 3105x4992x4=62MB array
	; we have ndeps=11 => single parser bytes = 3492
	; 3492x4992 = 17MB parser array => need ~100 MB per sentence

	;;; most of parser space (3200) spent on ldep/rdep, can we save?
	; ldep[h,i]=d so what is the max number of children? can we push?
	; start with an array of empty arrays and push when necessary.
	; TODO: if we push we can get rid of lcnt,rcnt as well.
	; could lose speed, profile for this change.
	; do we need ldep?  it is only used in features.jl and we can have
	; a little for loop there. if lcnt>0.

	;;; stats from zn11beam4.net2 yatbazp.dev.jld4 first 10 sents:
	; uas: .9526 (zn11punc), lem: 0.2
	; out of 10 sentences,
	; 2 have gold path finish on top.
	; 8 have gold path in final beam, 2 lost it. (no early stop)
	; after normalization the stats for the top beam score:
	; n=10 m=0.333277 s=0.220646 q=[0.061805 0.133657 0.311918 0.475614 0.74421]
	; so approximating the top with 1.0 (perceptron) is not good

	;;; how about approximating the whole dist with the beam:
	; bottom of the beam log10-score statistics:
	; n=10 m=-3.08294 s=0.548609 q=[-4.05862 -3.54291 -2.87512 -2.66125 -2.10193]
	; we need a way to id the same state in difft beams. hash(bs.parser.head)?
	;; comparing beam sizes 1<<(0..10):
	; same hash appearing more than once.  derivation ambiguity?  A LOT!
	; assuming 1<<10 gives the full dist, here is the beam vs coverage:
	beam	pmass
	1	0.36064
	2	0.490636
	4	0.598386
	8	0.681686
	16	0.743913
	32	0.797305
	64	0.849904
	128	0.900778
	256	0.9423
	512	0.977804
	1024	1.0?

2015-07-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- read other papers doing structured nnet learning.
	-- figure out why our first attempt did not work?
	-- what is the relation to contrastive estimation, contrastive divergence, noise contrastive estimation, negative sampling?
	--- http://www.cs.cmu.edu/~nasmith/papers/smith+eisner.acl05.pdf
	--- http://demo.clab.cs.cmu.edu/cdyer/nce_notes.pdf
	- replicate zn11:
	-- need structured perceptron working on sparse vectors.
	- implement collins2015: pnet: nnet+perceptron as in weiss15.
	-- pnet is not a regular net, need to introduce model.
	-- need to modify kuparser to work with pnet:
	--- call model predict, provide pnet training set
	--- need correct answers for averaging: package xy efficiently, rethink perceptorn train
	- try pnet with optimized nets from earlier experiments	(o,g,bparse)
	-- need to run gparse/oparse experiments.
	- feature optimization?  using o,g,bparse?
	- perceptron signal to nnet training did not work?
	-- try on smaller problems, collins2002.
	-- try after good initialization
	-- careful about adagrad,
	-- careful no averaging! implement ASGD.
	- better oracle accuracy using avg or momentum possible?
	-- implement ASGD in update.jl.  how?


2015-07-14  Deniz Yuret  <dyuret@ku.edu.tr>

	* collins2015:
	- based on chen&manning2014
	- do they use or learn embeddings?
	- collins attaches a perceptron on top of a oracle trained nnet
	- nobody fixes the feature set so feature optimization is ok
	- there are other references for structured nnet learning
	- adding perceptron adds ~0.6%
	- they also use additional data for another +1%
	- they completely freeze the nnet before training the perceptron
	- they use momentum and l2reg, train on gold parse trees only
	- they also use parameter averaging for nnet training?  ASGD (Bottou 2010)
	- they decrease lr every gamma rounds of updates.
	- should we try structured kernel perceptron?
	- they did not try fine-tuning during perceptron training.
	- we should try starting with a pretrained net before switching to
	perceptron updates.  if not try the separate perceptron idea.  and
	try ASGD.

	* test/train.jl: Trying perceptron loss with nnet training:
	Batch training doesn't get off the ground with
	early stop.  We need to start learning in small batches so we can
	parse further into the training sentences.

	Even if we implement structured learning, mmul does not do
	averaging.  Either need to add averaging mmul? or write a
	separate structured perceptron.

	Bparse does not work with sparse features in xy mode.

	Perceptron does not work with negative example training.

2015-07-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	search or space or objective?
	compare move accuracy with gparser oparser
	try nesterov if search problem
	check weights and try maxnorm if regularization problem.
	move accuracy = parse accuracy?
	can replicate zn11?  using sparse perceptron?
	figure out thrust speed: done
	release kunet kuparser
	create paper repo

	* src/bparser.jl: (beamdebug version) prints out the beam state at
	every move.  Except for very long sentences, correct parse is
	often in the beam.  Most move probabilities are very very close to
	1.  The differences are due to one or two moves.  Perceptron wins
	because it punishes the wrong path?  We could update with a
	uniformly distributed y?  Background distribution of y?  Is this
	similar to contrastive divergence?

	Use y for mincost and z for maxscore state/move.  In our training,
	we just pass the y state and move as training example.  No
	feedback on the z state and move (crucial difference from
	structured perceptron).  Whether or not z move is the
	right (mincost) move for the z state, its score should be lowered?
	It can still be kept the maxscore move but the model can be made
	less sure, make the z moves high entropy, closer to uniform dist.
	If the z state is generally a bad state we want all its scores to
	be low.  But they are normalized probabilities, so the best we can
	do is to approach uniform.  The fact that our scores are
	normalized does not help when y and z have different states: they
	are normalized by different constants.  Increasing the probability
	of the y move does not automatically lower that of the z move.  If
	we see these as partial scores for the graph did we just make the
	y graph more probable without lowering the z graph?  (we lowered
	the graphs reached from alternative y moves).

	ZN11 uses perceptron.  Perceptron does not normalize its scores.
	So it suffers from the normalization problem Smith mentions.  But
	that does not seem to be the important part.  Label bias
	etc. comes from not lack of normalization but the locality of the
	scoring model?  So structured perceptron avoids the locality?
	Beam search decoder or Viterbi solve locality problem during
	decoding?  So the problem is in training?

	Compare HMM, CRF, conditional MM (MeMM?), and structured
	perceptron on simple tagging task, learning and decoding.  Read
	the papers (smith and johnson 2007?).

	* archive:
	Moving gn13dynamic3.out to log/07082253-gn13dynamic3.out
	Moving gn13dynamic4.out to log/07091602-gn13dynamic4.out
	Moving gn13dynamic5.out to log/07091747-gn13dynamic5.out
	Moving gn13static3.out to log/07082106-gn13static3.out
	Moving gn13static4.out to log/07091513-gn13static4.out
	Moving gn13static5.out to log/07091655-gn13static5.out
	Moving sleep_1d.out to log/07102259-sleep_1d.out
	Moving sleep_1h.out to log/07071304-sleep_1h.out
	Moving zn11beam4.out to log/07100058-zn11beam4.out
	Moving zn11beam5.out to log/07101849-zn11beam5.out
	Moving zn11beam6.out to log/07121432-zn11beam6.out
	Moving zn11dynamic3.out to log/07090005-zn11dynamic3.out
	Moving zn11dynamic4.out to log/07091637-zn11dynamic4.out
	Moving zn11dynamic5.out to log/07091827-zn11dynamic5.out
	Moving zn11static3.out to log/07082156-zn11static3.out
	Moving zn11static4.out to log/07091536-zn11static4.out
	Moving zn11static5.out to log/07091757-zn11static5.out


2015-07-09  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE: check on *5.out for variance and effect of shuffling.

	* DONE:
	x reset adagrad to see if learning resumes?
	+ try shuffling and make it the default if it helps (train9.jl, *5.out)

2015-07-08  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	x Do 5x experiments with eager static and dynamic.
	+ reread a tale of two parsers and try to understand structured vs transition.  is gn13 structured?  is structured the same as beam, i.e. not greedy?
	- feature selection?
	- figure out the thrust tanh mistery in ~/cuda.
	- setup minecraft.
	- compare training curves (move/parse) for oparse, gparse, bparse.

	* zn11beam: in buffer *zn11beam*
	07081529-zn11beam-epochs-1-12.log: first 12 epochs (epoch 3 crashed, is empty)
	yatbazp.nnet.jld
	[dy_052@hpc3021 test]$ for i in `seq 1 20`; do echo "EPOCH $i"; julia --machinefile foo.machinefile5x6 train.jl --epochs 1 --seed $i --in yatbazp.nnet.jld --out yatbazp.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4 ; done
	psub -N zn11bparser2 `pwd`/train20.sh  --parser bparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --in yatbazp.nnet.jld --out yatbazp.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4
	12567903.hpc-pbs.hpcc.usc.edu
	hpc3007
	ssh hpc3007 tail /var/spool/torque/spool/*.OU
	THIS IS PROBABLY TOO SLOW KILLING IT and restarting 5x6...
	log/07081613-zn11bparser2.out
	[dy_052@hpc-login3 test]$ psub -N zn11bparser3 `pwd`/train20m.sh  --parser bparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --in yatbazp.nnet.jld --out yatbazp.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4
	12567954.hpc-pbs.hpcc.usc.edu
	running on head=hpc3007, workers=3002,3008,3009,3010,3011
	ssh hpc3007 grep DATA /var/spool/torque/spool/*.OU

	Result: 07081529-zn11beam-epochs-1-12.log + 07090417-zn11bparser3.out
	- trn drops after epoch 18 .9630
	- dev drops after epoch 12 .9305
	- tst drops after epoch 13 .9292
	- Replicate to see variance:
	psub -N zn11beam4 `pwd`/train20m.sh  --parser bparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --hidden 4096 4096 --dropout .1 .5 --in zn11beam4.net --out zn11beam4.net yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4
	- Should have been:
	psub -N zn11beam4 `pwd`/train20m.sh  --parser bparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --hidden 4096 4096 --dropout .1 .5 --in zn11beam4.net --out zn11beam4.net --ispunct zn11punct yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4
	TODO: analyze results (also shuffling)

	- zn11beam5: after bugfix
	psub -N zn11beam5 `pwd`/train20m.sh --parser bparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --hidden 4096 4096 --dropout .1 .5 --in zn11beam5.net --out zn11beam5.net --ispunct zn11punct yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4

	- zn11beam6: *julia* with earlystop
	nohup ./train20dbg.sh --parser bparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --hidden 4096 4096 --dropout .1 .5 --in zn11beam6.net --out zn11beam6.net --ispunct zn11punct yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4 > zn11beam6.out 2> zn11beam6.err

	- Does not seem to improve.  New hypothesis?

	- @@zn11beam7: with structured learning
	psub -N zn11beam7 `pwd`/train20m.sh --parser bparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --hidden 4096 4096 --dropout .1 .5 --in zn11beam7.net --out zn11beam7.net --ispunct zn11punct yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4

	* zn11dynamic:
	foo.hpc3004.log
	hpc3004: zn11dynamic: careful, pick the score with punctuation!
	[dy_052@hpc3004 test]$ julia -p 4 train.jl --parser gparser --arctype ArcEager13 --feats zn11cpv --hidden 4096 4096 --dropout .1 .5 --epochs 1 --seed 42 --out gparser.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4
	[dy_052@hpc3004 test]$ for i in `seq 2 20`; do echo "EPOCH $i"; julia -p 4 train.jl --parser gparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --seed $i --out gparser.nnet.jld --in gparser.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4; done
	07081635-zn11dynamic-epochs-1-20.log
	gparser.nnet.jld
	[dy_052@hpc-login3 test]$ psub -N zn11dynamic2 `pwd`/train20.sh  --parser gparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --in gparser.nnet.jld --out gparser.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4
	12568315.hpc-pbs.hpcc.usc.edu
	BUGGY: we should be using the conll07 dataset!
	[dy_052@hpc-login3 test]$ psub -N zn11dynamic3 `pwd`/train40.sh  --parser gparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --in zn11dynamic.net --out zn11dynamic.net conll07.trn.jld4 conll07.tst.jld4 
	12569034.hpc-pbs.hpcc.usc.edu
	ssh hpc3004 grep evalparse /var/spool/torque/spool/*.OU

	Result:
	- trn still going up at epoch 40 .9180
	- tst zig-zags after epoch 7 between .884 and .894 with average .888

	- Run2 to see variation
	psub -N zn11dynamic4 `pwd`/train20.sh  --parser gparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --hidden 4096 4096 --dropout .1 .5 --in zn11dynamic4.net --out zn11dynamic4.net conll07.trn.jld4 conll07.tst.jld4 
	12570850.hpc-pbs.hpcc.usc.edu
	ssh hpc3001 grep evalparse /var/spool/torque/spool/*.OU
	psub -N zn11dynamic4 `pwd`/train20r.sh  --parser gparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --hidden 4096 4096 --dropout .1 .5 --in zn11dynamic4.net --out zn11dynamic4.net conll07.trn.jld4 conll07.tst.jld4
	ssh hpc3020 grep DATA $OU
	8934 and climbing.

	- zn11dynamic5 is the shuffling version. comparison not significant:
		15	20	best
	noshuf4	8919	8919	8977@17
	shuf5	8923	8927	8947@9

	* zn11static:
	[dy_052@hpc3005 test]$ julia -p 4 train.jl --parser oparser --arctype ArcEager13 --feats zn11cpv --hidden 4096 4096 --dropout .1 .5 --epochs 1 --seed 42 --out oparser.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4
	[dy_052@hpc3005 test]$ for i in `seq 2 20`; do echo "EPOCH $i"; julia -p 4 train.jl --parser oparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --seed $i --in oparser.nnet.jld --out oparser.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4; done
	oparser.nnet.jld
	07081557-zn11static-epochs-1-20.log
	[dy_052@hpc-login3 test]$ psub -N zn11static2 `pwd`/train20.sh --parser oparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --in oparser.nnet.jld --out oparser.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4
	12567949.hpc-pbs.hpcc.usc.edu
	ssh hpc3005 tail /var/spool/torque/spool/*.OU
	BUGGY: use the conll07 dataset!
	[dy_052@hpc-login3 test]$ psub -N zn11static3 `pwd`/train40.sh  --parser oparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --in zn11static.net --out zn11static.net conll07.trn.jld4 conll07.tst.jld4 
	12569040.hpc-pbs.hpcc.usc.edu
	ssh hpc3005 grep evalparse /var/spool/torque/spool/*.OU

	Result:
	- tst peaks at .8668 epoch 13, average after epoch 10: .8654.
	- Replicating for variation: zn11static4: converged to .8659.  This is identical, no variation?
	- wrote train20r.sh that sets a random seed: rerunning zn11static4.
	psub -N zn11static4 `pwd`/train20r.sh  --parser oparser --arctype ArcEager13 --feats zn11cpv --epochs 1 --hidden 4096 4096 --dropout .1 .5 --in zn11static4.net --out zn11static4.net conll07.trn.jld4 conll07.tst.jld4 
	ssh hpc3021 grep DATA $OU
	.8755: seed helped, shuffling?
	- zn11static5 is the shuffling version.  comparison not significant:
		15	20	best
	noshuf4	8793	8775	8793
	shuf5	8767	8813	8813

	* gn13dynamic:
	[dy_052@hpc3006 test]$ julia -p 4 train.jl --parser gparser --arctype ArcHybrid13 --feats gn13cpv --hidden 4096 4096 --dropout .1 .5 --epochs 1 --seed 42 --out gn13gparser.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4
	[dy_052@hpc3006 test]$ for i in `seq 2 20`; do echo "EPOCH $i"; julia -p 4 train.jl --parser gparser --arctype ArcHybrid13 --feats gn13cpv --epochs 1 --seed $i --out gn13gparser.nnet.jld  --in gn13gparser.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4; done
	07081642-gn13dynamic-epochs-1-20.log
	[dy_052@hpc-login3 test]$ psub -N gn13dynamic2 `pwd`/train20.sh --parser gparser --arctype ArcHybrid13 --feats gn13cpv --epochs 1 --out gn13gparser.nnet.jld  --in gn13gparser.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4
	12568324.hpc-pbs.hpcc.usc.edu
	BUGGY: restart with conll07 data
	[dy_052@hpc-login3 test]$ psub -N gn13dynamic3 `pwd`/train40.sh --parser gparser --arctype ArcHybrid13 --feats gn13cpv --epochs 1 --out gn13dynamic.net --in gn13dynamic.net conll07.trn.jld4 conll07.tst.jld4 
	12569043.hpc-pbs.hpcc.usc.edu
	ssh hpc3006 grep evalparse /var/spool/torque/spool/*.OU

	Results:
	- peaks at epoch 9 at .8760 (seen only once).  avg(3:40)=.8690
	- previous best results with hybrid13 (no pos) at .8827 max, .8766 avg
	- changing features gets us .88, .89.
	- @@Replicating to see variance at gn13dynamic4.
	psub -N gn13dynamic4 `pwd`/train20r.sh --parser gparser --arctype ArcHybrid13 --feats gn13cpv --epochs 1 --hidden 4096 4096 --dropout .1 .5 --out gn13dynamic4.net --in gn13dynamic4.net conll07.trn.jld4 conll07.tst.jld4
	ssh hpc3004 grep DATA $OU
	Seeing .8951, random seed definitely helped.  Maybe should try shuffling?
	- gn13dynamic5 is the shuffling version.
		15	20	best
	noshuf4	8995	8977	9017@16
	shuf5	8957	8963	8973@16

	* gn13static: hpc3017: psub:
	main queue: psub -q default -N gn13oparser `pwd`/train20.sh --parser oparser --arctype ArcHybrid13 --feats gn13cpv --epochs 1 --out gn13oparser.nnet.jld  --in gn13oparser.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4
	12566917.hpc-pbs.hpcc.usc.edu
	isi queue: psub -N gn13oparser2 `pwd`/train20.sh --parser oparser --arctype ArcHybrid13 --feats gn13cpv --epochs 1 --out gn13oparser.nnet.jld  --in gn13oparser.nnet.jld yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4
	12566925.hpc-pbs.hpcc.usc.edu
	ssh hpc3017 grep evalparse /var/spool/torque/spool/*.OU
	BUGGY: restart with conll07 data
	[dy_052@hpc-login3 test]$ psub -N gn13static3 `pwd`/train40.sh --parser oparser --arctype ArcHybrid13 --feats gn13cpv --epochs 1 --out gn13static.net --in gn13static.net conll07.trn.jld4 conll07.tst.jld4 
	12569045.hpc-pbs.hpcc.usc.edu
	ssh hpc3012 grep evalparse /var/spool/torque/spool/*.OU

	Results:
	- Peaks at .8829 at epoch 24. avg(11:40)=.8820
	- Weirdly higher than dynamic?
	- Previous best (no pos) was at 8823 max, 8728 avg vs dynamic at 8827 max, 8766 avg.
	- So no surprize here.
	- Replicating to see variance at gn13static4.
	psub -N gn13static4 `pwd`/train20r.sh --parser oparser --arctype ArcHybrid13 --feats gn13cpv --epochs 1 --hidden 4096 4096 --dropout .1 .5 --out gn13static4.net --in gn13static4.net conll07.trn.jld4 conll07.tst.jld4
	ssh hpc3001 grep DATA $OU
	Seeing .8910 => seeding each epoch helped?
	- gn13static5 is the shuffling version.
		15	20	best
	noshuf4	8891	8883	8913@17
	shuf5	8867	8919	8955@16


	* Data: sort through the confusion with the data:
	yatbaz: gold postags
	yatbazp: predicted postags
	acl11*, zn11*: anything before july 6: train has predicted, dev/test have gold postags.
	at this point it makes sense to use the old acl11 results up to the parsing experiments.

	* zn11nnet7: Reoptimize nnet for conll07.
	Created conll07-zn11cpv.jld
	[dy_052@hpc-login3 test]$ psub julia zn11nnet7.jl conll07-zn11cpv.jld .01 .1 .5 4096 4096
	12570094.hpc-pbs.hpcc.usc.edu
	for i in `myqueue | perl -lne 'print $1 if /zn11nnet7.*(hpc\d+)/'`; do ssh $i grep Name /var/spool/torque/spool/*.OU; ssh $i grep epoch /var/spool/torque/spool/*.OU | sort -gr -t, -k6 | head -1; done
	Result: the original setting still the best.

	maxtst	lr	d1	d2	h
	.9603	julia_zn11nnet7_jl_conll07_zn11cpv_jld_01_1_5_4096_4096
	.9602	julia_zn11nnet7_jl_conll07_zn11cpv_jld_01_1_3_4096_4096
	.9598	julia_zn11nnet7_jl_conll07_zn11cpv_jld_01_1_5_8192_8192
	.9597	julia_zn11nnet7_jl_conll07_zn11cpv_jld_01_1_5_2048_2048
	.9590	julia_zn11nnet7_jl_conll07_zn11cpv_jld_02_1_5_4096_4096
	.95875	julia_zn11nnet7_jl_conll07_zn11cpv_jld_01_2_5_4096_4096
	.95875	julia_zn11nnet7_jl_conll07_zn11cpv_jld_01_1_5_11585
	.9584	julia_zn11nnet7_jl_conll07_zn11cpv_jld_01_0_5_4096_4096
	.9584	julia_zn11nnet7_jl_conll07_zn11cpv_jld_005_1_5_4096_4096
	.9582	julia_zn11nnet7_jl_conll07_zn11cpv_jld_01_1_5_23170
	.9581	julia_zn11nnet7_jl_conll07_zn11cpv_jld_01_1_5_16384
	.9578	julia_zn11nnet7_jl_conll07_zn11cpv_jld_01_1_5_32768
	.9568	julia_zn11nnet7_jl_conll07_zn11cpv_jld_01_1_7_4096_4096

	* zn11nnet: checking nnet settings with yatbazp-zn11cpv.jld
	for i in `myqueue | perl -lne 'print $1 if /zn11nnet.*(hpc\d+)/'`; do ssh $i grep Name /var/spool/torque/spool/*.OU; ssh $i grep epoch /var/spool/torque/spool/*.OU | sort -gr -t, -k6 | head -1; done
	Result: lr=.01 dropout=.1,.5 hidden=4096,4096 adagrad=1e-8 still good.

	maxdev	lr	d1	d2	h
	.9708	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_01_1_5_4096_4096
	.97045	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_01_2_5_4096_4096
	.9704	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_01_0_5_4096_4096
	.97035	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_01_1_5_2048_2048
	.9703	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_005_1_5_4096_4096
	.9700	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_02_1_5_4096_4096
	.9699	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_01_1_5_8192_8192
	.9697	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_01_1_5_11585
	.9696	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_01_1_3_4096_4096
	.9694	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_01_1_5_23170
	.9694	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_01_1_5_16384
	.96905	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_01_1_7_4096_4096
	.96905	julia_zn11nnet_jl_yatbazp_zn11cpv_jld_01_1_5_32768

	* test/zn11nnet.jl: The 2015-07-05 experiments were done on gold
	tags.  Replicating for predicted tags:

	.9708	.01 .1 .5 4096 4096	log/07072207-julia_zn11nnet_jl_yatbazp_zn11cpv_jld_01_1_5_4096_4096.out

	* test/zn11nnet.jl: Replacing nnet results with better nnet.
	Let's also fix the postag problem and use gold postags (yatbaz-*)
	for these experiments.  yatbazp-* have predicted postags.  The
	original experiments with zn11* came from acl11.zip and had mixed
	postags, predicted for train, gold for dev/test.  Let's use
	predicted for everything, or gold for machine learning, predicted
	for parsing?  We'd need to recreate sparse data as well.
	@@psub julia zn11nnet.jl zn11pv.jld .01 .1 .5 4096 4096
	@@psub julia zn11nnet.jl zn11cpv.jld .01 .1 .5 4096 4096

2015-07-07  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	+ prep data with predicted/gold postags and yatbaz embedding:
	yatbazp.*: cofirmed using compare-jld4.jl
	+ fix bparser xy problem.
	+ fix worker memory reset problem.
	+ figure out optimum nbatch, workers/machine.
	+ run bparser experiment.
	x resort to feature selection.
	- experiment with early stop.
	- add a punct option in train.jl to determine what score to use.
	+ old results suspect because i used predicted postags in
	  acl11.trn.jld but gold ones in acl11.dev.jld.
	  creating yatbazp-zn11cpv.jld
	  psub julia zn11nnet.jl yatbazp-zn11cpv.jld .01 .1 .5 4096 4096

	* test/train.jl: After a long debugging session, rewrote the whole
	thing.  The x matrix gets too big.  So we parse training corpus in
	chunks, feeding one chunk at a time to parsers, then feeding the
	results immediately to train.  Never keep around big chunks.
	@@kunet

	* Timing: for sentence chunk train:2501:5000
	$ julia --machinefile foo.machinefile8x4 train.jl --parser bparser --nbeam 64 --arctype ArcEager13 --feats zn11cpv --hidden 4096 4096 --dropout .1 .5 --epochs 20 --seed 42 yatbazp.trn.jld4 yatbazp.dev.jld4 yatbazp.tst.jld4 

	machine	worker	sbatch	pbatch	bparse	train
	3	8	2500	25	98	41
	5	4	2500	25	93	35
	5	6	2500	25	84	41	# optimal
	5	8	2500	25	88	42
	5	10	2500	25	88	46
	5	12	2500	25	97	51
	5	16	2500	32	113	55
	8	2	2500	25	100	34	# seems machine x worker < 25 not good enough
	8	4	2500	25	84	40	# optimal
	8	8	2500	25	107	52	# seems machine x worker > 40 slowing us down
	4	8	2500	25	114	40	# just machine x worker not enough


	with 3 machines x 8 workers and 2500 sent chunks (16 train chunks):

	epoch 1:
	2015-07-07T16:15:53 p = oparse(pt,sentences,ndeps,args["ncpu"],feats)
	oparse(1:2500) 14 secs
	train(1:2500) 41 secs
	2015-07-07T16:30:29 p = bparse(pt,sentences,ndeps,feats,net,args["nbeam"],args["pbatch"],args["ncpu"]; xy=false)
	bparse(1700) 78 secs
	2015-07-07T16:31:48 p = bparse(pt,sentences,ndeps,feats,net,args["nbeam"],args["pbatch"],args["ncpu"]; xy=false)
	bparse(2416) 92 secs

	epoch 2:
	2015-07-07T16:33:21 p = bparse(pt,sentences,ndeps,feats,net,args["nbeam"],args["pbatch"],args["ncpu"]; xy=true)
	bparse(2500) 98 secs
	train(2500) 41 secs
	2015-07-07T17:10:45 p = bparse(pt,sentences,ndeps,feats,net,args["nbeam"],args["pbatch"],args["ncpu"]; xy=false)
	bparse(dev1700) 72 secs
	2015-07-07T17:11:57 p = bparse(pt,sentences,ndeps,feats,net,args["nbeam"],args["pbatch"],args["ncpu"]; xy=false)
	bparse(tst2416) 89 secs


	Compare with 5 machines x 8 workers and 2500 sent chunks:
	bparse(2500) 83 secs
	train(2500) 42 secs

	Compare with 5 machines x 16 workers and 2500 sent chunks, pbatch=32 (was 25)
	bparse(2500): 113 secs
	train(2500): 55 secs

	Compare with 5 machines x 4 workers and 2500 sent chunks: (back to pbatch=25)
	bparse(2500): 93 secs
	train(2500): 35 secs

2015-07-06  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/resetworkers.jl: gc() still does not work in v0.3.  With our
	without DArrays, I could not find a way to gc() an array returned
	from the worker consistently.  We'll have to use
	restartmachines().  There may or may not be any reason to use
	DArrays for anything other than the input.

2015-07-05  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/zn11nnet.jl: tune nnet for 20 epoch runs:
	lr, drop1, drop2, hidden1, hidden2...

	log/*-julia_zn11nnet_jl_zn11cpv_jld_*.out

	.9678	0.01562 0 0 2048 2048
	.9674	0.01562 0 0 4096 4096
	.9678	0.01562 0 0 8192 8192

	.9692	0.01562 0 0 11585
	.9691	0.01562 0 0 16384
	.9694	0.01562 0 0 23170

	.9695	0.03125 .2 .5 16384
	.9698	0.01562 .2 .5 16384
	.9700	0.00781 .2 .5 16384

	.9704	0.01562 .2 .5 2048 2048
	.9708	0.01562 .2 .5 8192 8192

	.9708	0.00390625 .2 .5 4096 4096
	.9707	0.005 .2 .5 4096 4096
	.9709	0.00781 .2 .5 4096 4096
	.9710	0.01 .2 .5 4096 4096	*****
	.9704	0.01562 .2 .5 4096 4096
	.9704	0.03125 .2 .5 4096 4096
	.9695	0.06250 .2 .5 4096 4096

	.9717	0.01 .1 .5 4096 4096	*****
	.9704	0.01 .3 .5 4096 4096
	.9710	0.01 .2 .5 4096 4096
	.9705	0.01 .2 .3 4096 4096
	.9696	0.01 .2 .7 4096 4096

	.9715	0.01 0 .5 4096 4096
	.9704	0.01 .1 .3 4096 4096
	.9717	0.01 .1 .5 4096 4096	*****
	.9698	0.01 .1 .7 4096 4096

	* testbparser3.jl: parallelism.
	cannot get speedup using one machine and multiple gpu/cpu.

	100 sentence test:
	1 machine with xy: 45 secs
	5 machines x 2 workers x nbatch=10 with xy: 17 secs / 22 secs with restart

	250 sentence test:
	single cpu: 121 secs
	5 machines: 39 secs
	5 machines x 2 workers each: 35 secs

	500 sentence test:
	5 machines x 2 workers each: 36 secs

	1000 sentence test:
	5 machines x 1 worker each: 111 secs
	5 machines x 2 workers each: 60 secs
	5 machines x 4 workers each: 50 secs

	1700 sentence test (whole dev):
	5 machines x 2 workers each: 92 secs
	5 machines x 4 workers each: 72 secs
	5 machines x 8 workers each: 72 secs

	These were for nbatch=5.
	5 machines x 4 workers x nbatch=10: 70 secs
	5 machines x 4 workers x nbatch=20: 69 secs
	5 machines x 4 workers x nbatch=85: 69 secs

	However this was with default batch=128 for predict.  Try again
	with unlimited predict batch size.

	5 machines x 4 workers x nbatch=85: 65 secs

	gc() may be slowing down, commenting out gc() in train.  That's
	stupid, we are not doing train!  65 secs is it.

	5 machines x 4 workers x nbatch=2: 72 secs

	There is no point in using a large nbatch.

	5 machines x 6 workers x nbatch=10: 65 secs
	5 machines x 8 workers x nbatch=10: 68 secs

	5000 sentence experiments:

	5 machines x 8 workers x nbatch=5: 136 secs
	5 machines x 8 workers x nbatch=25: 126 secs
	5 machines x 10 workers x nbatch=25: 121 secs
	5 machines x 12 workers x nbatch=25: 122 secs
	5 machines x 16 workers x nbatch=5: 133 secs

	10000 sentence experiments:
	5 machines x 16 workers x nbatch=5: 202 secs
	5 machines x 16 workers x nbatch=25: 187 secs

	10000 sentences with xy:
	(30 workers, 334 sentences per worker)
	15 machines x 2 workers x nbatch=5: 224 secs
	15 machines x 2 workers x nbatch=25: 216 secs
	15 machines x 2 workers x nbatch=335: 217 secs
	(60 workers, 167 sentences per worker)
	15 machines x 4 workers x nbatch=5: 193 secs
	15 machines x 4 workers x nbatch=25: 189 secs
	15 machines x 4 workers x nbatch=56: 179.6 secs
	(120 workers, 84 sentences per worker)
	15 machines x 8 workers x nbatch=5: 207 secs
	15 machines x 8 workers x nbatch=21: 212 secs
	15 machines x 8 workers x nbatch=44: 212 secs

	full training set experiments:
	5 machines x 16 workers x nbatch=25: 576 secs
	15 machines x 6 workers x nbatch=25: 483 secs
	15 machines x 6 workers x nbatch=125: oom
	15 machines x 8 workers x nbatch=25: 480 secs (10 secs to hcat x, 333 sent/worker)
	15 machines x 16 workers x nbatch=25: oom

	WARNING: with xy, the time is dominated by transferring x! (~200 secs)

	* src/bparser.jl: DONE: profile bparser.  Using net
	"foo11nnet3.jld" trained single epoch with "zn11cpv.jld 0.01 .2 .5
	4096 4096".  accuracy: 0.9606554389983601,0.9606121471216923.

	bparser methods:
	bparse(pt, c, ndeps, feats, net, nbeam, [nbatch=1]) # single cpu
	bparse(pt, c, ndeps, feats, net, nbeam, nbatch, ncpu) # multi cpu
	bparse(pa, c, ndeps, feats, net, nbeam, nbatch, [x, y, nx]) # workhorse

	testbparser3.jl
	bparse(pt, corpus[1:10], ndeps, ft, net, nbeam;	xy=true) # 6509
	features(b.parser[i], b.sentence, feats, f, (nf+=1)) 	# L106: 2562
	cost = movecosts(b.parser[i], b.sentence.head, ...)  	# L107: 575
	predict(net, sub(f,:,1:nf), sub(score,:,1:nf))       	# L122: 2755
	b.cscore[nc] = b.pscore[i] + score[j,nf]		# L135:	179
	sortpermx(sub(b.csorted, 1:nc), sub(b.cscore, 1:nc); rev=true)	# L139: 146

	Nothing obvious from profiling.

	Effect of batch size: almost none at nbeam=64.
	128 sentences take 55-56 secs from nbatch=128 to nbatch=2, 61 secs
	without nbatch.  So small batch sizes saving memory is ok.  Memory
	used is 560MB.

	Effect of multi-cpu: does not speed up as expected on a single
	machine.

	Try multi-machine: fails with cublas.  starting with CUDArt,
	CUBLAS, CUDNN, KUnet, KUparser, test multi-machine before
	bparser.  It turns out cublas is failing when using more than one
	gpu per machine.

	DONE: do we drop during testing? predict flag takes care.

	DONE: multi-cpu uses shared-array, which will not work on multiple
	machines! got rid of shared arrays.

2015-07-04  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	TODO: tune feature set
	TODO: profile parsers
	TODO: run gparser?
	TODO: use multi-machine parallel for bparser

2015-07-03  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/zn11pvc.jl: TODO: rerun using nnets.

	* test/zn11pvc.jl: collected results to spreadsheet and archived.

2015-07-01  Deniz Yuret  <dyuret@ku.edu.tr>

	* acl11: check postag difference with acl11.
	It seems devi/devo, testi/testo, and train have predicted postags.
	devr and testr have gold postags.

	[dy_052@hpc3007 data]$ for i in acl11 cw hlbl glove mikolov random yatbaz mikolov300; do echo $i; julia compare-jld4.jl bansal.trn.jld4 $i.trn.jld4; done
	acl11
	a["deprel"] == b["deprel"] => true
	a["postag"] == b["postag"] => true
	aform == bform => false
	apostag == bpostag => false
	ahead == bhead => true
	adeprel == bdeprel => true
	awvec == bwvec => false


	* bansal-zn11pv.jld:
	- embeddings.sh, uses conll7jld.jl, creates jld4 files.
	- julia getfeatures.jl bansal zn11pv

	* kunet: embeddings.sh converted to jld4.

	* shell: embeddings copied from balina.

	* @@parser: TODO: memory and time problems with bparser for the
	final parsing experiments.

	* train.jl: TODO:
	- check for early stop.
	- removed dropout, what is the effect?
	- repeat feature selection experiments.

	* zn11pvcnet.jl: pvc nnet experiments using 16K hidden and 0.01562
	lr.  single hidden layer.  no dropout.

	* datafiles:
	- dense:
	zn11c.jld
	zn11cp.jld
	zn11cpv.jld
	zn11cv.jld
	zn11n.jld
	zn11p.jld
	zn11pv.jld
	zn11v.jld
	- sparse:
	zn11oparse1.jld: zn11single (39) feature set
	zn11oparse.jld: zn11orig (72) feature set


2015-06-30  Deniz Yuret  <dyuret@ku.edu.tr>

	* 06302022-julia_zn11nnet4_jl_zn11cpv_jld.out: trying cpv.
	+ 07011657-julia_zn11nnet5_jl_zn11cpv_jld.out
	data	hidden	best-lr	best-dev
	cpv	128	0.12500 0.9598
	cpv	256	0.06250	0.9618
	cpv	512	0.06250	0.9631
	cpv	1024	0.03125	0.9640
	cpv	2048	0.03125	0.9648
	cpv	4096	0.03125	0.9651
	cpv	8192	0.01562	0.9653
	cpv	16384	0.01562	0.9658
	cpv	32768	0.00781	0.9655
	cpv	65536	0.00781	0.9657

	* 06302014-julia_zn11nnet4_jl_zn11cv_jld.out: trying cv.
	+ 07011608-julia_zn11nnet5_jl_zn11cv_jld.out
	data	hidden	best-lr	best-dev
	cv	128	0.06250	0.9522
	cv	256	0.06250	0.9555
	cv	512	0.03125	0.9568
	cv	1024	0.06250	0.9584
	cv	2048	0.03125	0.9590
	cv	4096	0.03125	0.9604
	cv	8192	0.01562	0.9607
	cv	16384	0.01562	0.9607
	cv	32768	0.01562	0.9609
	cv	65536	0.01562	0.9609

	* 06302002-julia_zn11nnet3_jl_zn11pv_jld.out: dropout with single 16K hidden layer.
	data	hidden	best-lr	best-dev
	pv	16384	0.03125	0.9597
	pv	16384	0.01562	0.9597

	* 06302159-julia_zn11nnet2_jl_zn11pv_jld.out: 2-hidden-layers
	data	2hidden	best-lr	best-dev
	pv	128	0.06250	0.9576
	pv	256	0.06250	0.9593
	pv	512	0.03125	0.9616
	pv	1024	0.03125	0.9617
	pv	2048	0.01562	0.9631
	pv	4096	0.01562	0.9629
	pv	8192	0.00781	0.9635

	* 07010942-julia_zn11nnet_jl_zn11pv_jld.out: optimizing lr and hidden with 1-epoch training.
	data	hidden	best-lr	best-dev
	pv	128	0.06250	0.9578
	pv	256	0.06250	0.9600
	pv	512	0.06250	0.9607
	pv	1024	0.03125	0.9622
	pv	2048	0.03125	0.9622
	pv	4096	0.03125	0.9628
	pv	8192	0.01562	0.9629
	pv	16384	0.01562	0.9635
	pv	32768	0.01562	0.9632
	pv	65536	0.01562	0.9635

	* acl11trn.jld4: TODO: separate wvec and cvec into two features.

2015-06-29  Deniz Yuret  <dyuret@ku.edu.tr>

	* out-of-mem: gpu/dense gave out-of-memory error.
	- find out why
	- try 1.33x realloc (the 4th realloc can use space from 1+2)
	- try prealloc
	- try uniq
	- try sv id

	sv:
	zn11n:   165 x 788607  = 130m (epoch 1)
	zn11p:   705 x 648569  = 457m (epoch 7)
	zn11v:  1365 x 288501  = 394m (epoch 6)
	zn11c:  1365 x 232284  = 317m (epoch 1)
	zn11pv: 1905 x 304644  = 580m (epoch 10)
	zn11cp: 1905 x 306482  = 584m (epoch 6)
	zn11cv: 2565 x 119377  = 306m (epoch 1)
	zn11cpv: 3105 x 108499 = 337m (epoch 1)

	zn11n probably explodes because of k size: 128 (xbatch) x 788607.

	* TODO: check speed of gpumem()

2015-06-28  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/zn11pvc.jl (net): Determining optimal gamma for pvc
	experiments (8 subsets of postag, wvec, cvec).  The datasets were
	generated using acl11features.jl.

	gamma:
	zn11n: 0.40
	zn11p: 0.17
	zn11v: 0.25
	zn11c: 0.30
	zn11pv: 0.14
	zn11cp: 0.18
	zn11cv: 0.16
	zn11cpv: 0.12

	* DONE: profiling?
	* TODO: do learning rate experiments (one-hot vs dense)

2015-06-26  Deniz Yuret  <dyuret@ku.edu.tr>

	* plan:
	Experiment 1: nonlinear vs linear (using perceptron)
	Experiment 2: sparse vs dense (using nonlinear)
	Experiment 3: perceptron vs nnet (using dense)
	Experiment 4: scode vs other (using nnet)
	Experiment 5: postags vs context vectors (using scode)

	* test/zn11perc64.jl: early convergence of regular perceptron is
	probably due to numerical accuracy.  Trying with Float64.  Fixed
	it.  Also added the (seed>0) condition to see unshuffled results.

	* TODO: run zn11orig with kperc and zn11single with perc as well.

	06261347-julia_zn11perc64_jl.out: perc+zn11orig
	- does not converge in 20 epochs but dev peaks.
	- does worse than Float32?
	best trn: (0,20,0.990577853561211,0.9628940312882318,0.9628049679369057)
	best dev: (0,12,0.9864501711719235,0.9629200614311373,0.9633301393086166)
	best tst: (0,13,0.9871840790335269,0.9627118202878934,0.9634038475713127)

	06261523-julia_zn11perc64b_jl.out: perc+zn11single
	- does much worse but does not peak in 20.
	best trn: (0,20,0.9607095614570927,0.9493974021917381,0.9487543303604334)
	best dev: (0,18,0.9596867048415946,0.9494234323346435,0.9485792732365298)
	best tst: (0,20,0.9607095614570927,0.9493974021917381,0.9487543303604334)

	06261407-zn11test.pbs.o12464157: kperceptron+zn11single. (sparse)
	kgauss,gamma=0.12.
	(1,(338122,124946),0.9608506650701513,0.9600224810201223)	trn=0.9313631349731266	time=3405
	(2,(338122,162316),0.9627899107166098,0.9627404732070465)	trn=0.9794714544999099	time=6740
	(3,(338122,180291),0.963336543717625,0.9635604776295422)	trn=0.9901257531344897	time=7920
	(4,(338122,191068),0.9638701616471874,0.9638921648116754)	trn=0.9940798465385478	time=8540
	(5,(338122,198841),0.9641174480047896,0.9640487948699049)	trn=0.995730040562692	time=8941
	(6,(338122,204862),0.9640784027904313,0.964076435468416)	trn=0.996692470632699	time=9242
	(7,(338122,209836),0.9640133274331676,0.9638000294833051)	trn=0.9972676214793297	time=9480
	(8,(338122,214092),0.9641304630762423,0.9638184565489791)	trn=0.9976620420217184	time=9679
	(9,(338122,217855),0.9641695082906006,0.9639013783445124)	trn=0.9979328628119658	time=9850

	@@12501844@hpc3013 rerunning for epoch >= 10
	0.931363134973127	0.9608506650701513	0.9600224810201223
	0.97947145449991	0.9627899107166098	0.9627404732070465
	0.99012575313449	0.963336543717625	0.9635604776295422
	0.994079846538548	0.9638701616471874	0.9638921648116754
	0.995730040562692	0.9641174480047896	0.9640487948699049
	0.996692470632699	0.9640784027904313	0.964076435468416
	0.99726762147933	0.9640133274331676	0.9638000294833051
	0.997662042021718	0.9641304630762423	0.9638184565489791
	0.997932862811966	0.9641695082906006	0.9639013783445124
	0.998116889109598	0.9642215685764115	0.9639566595415346
	0.998195443618737	0.9642085535049587	0.9639566595415346
	0.99829102742706	0.9642736288622225	0.9640487948699049
	0.998389357896541	0.9644167946482026	0.96409486253409
	0.998423965827141	0.9645209152198245	0.9641501437311123
	0.998500323007352	0.9644167946482026	0.964085649001253
	0.998617330772713	0.9644688549340136	0.9640487948699049

	06271100-julia_zn11test2_jl.out: kperceptron+zn11orig. kgauss,gamma=0.05.
	(1,(5123080,115857),0.9610589062133951,0.960206751676863)	trn=0.9363560156274033	time=5981
	(2,(5123080,146889),0.9629981518598537,0.9627957544040687)	trn=0.982953122184672	time=11998
	(3,(5123080,159776),0.9637009657183018,0.9636341858922385)	trn=0.992920755529578	time=13849
	(4,(5123080,167105),0.964195538433506,0.9639750866072087) 	trn=0.995973944073584	time=14723
	(5,(5123080,172184),0.9646250357914465,0.9640211542713938)	trn=0.997209941594997	time=15260
	(6,(5123080,176178),0.9645339302912773,0.964076435468416) 	trn=0.997805967066434	time=15659

2015-06-25  Deniz Yuret  <dyuret@ku.edu.tr>

	* test/zn11test.pbs.o12463546: kperceptron on the zn11single
	feature set gives out of memory in epoch 2 with 125K sv while
	reallocating l.k.  (128*125K*4=64MB).  gc problem?  sv prealloc
	too aggressive?  stop reallocating l.k?  upload x and don't have
	sv?  sv costs ncols+2*nnz.  4*(125K+2*39*125K)=40MB.  Even double
	the size would be 80MB.  What is causing the error?  Do we need to
	call gc() during training?  Preallocate a huge sv, so we never
	alloc again?  Upload x would
	cost (2*nnz+1)*ncol*4=(2*39+1)*1.8M*4=569MB.  Would have to
	rewrite kernel.  Kernel caching would be easy using x indices.
	Shuffling would mess that up.  x could be indices too!  train
	could not handle it.  or train can just pass indices for x.  one
	hash (idx*idx->kval) plus a big x matrix in gpu memory for all the
	kernel calc.  what does a kernel cache hold again?  values or
	whole columns?  - It turns out gc() after every update fixes the
	problem.

	I calculated trn error looking at the number of sv added, and
	using the fact that nx=1820392.

	trn1: elapsed time: 3405.547185297 seconds (10105037976 bytes allocated, 9.32% gc time)
	dev1: elapsed time: 231.487048972 seconds (40981980 bytes allocated)
	tst1: elapsed time: 328.463520519 seconds (48170864 bytes allocated, 0.01% gc time)

	trn2: elapsed time: 6740.557072649 seconds (15279129024 bytes allocated, 4.77% gc time)
	dev2: elapsed time: 300.152283337 seconds (34083240 bytes allocated)
	tst2: elapsed time: 426.0930742 seconds (48177472 bytes allocated, 0.01% gc time)

	trn3: elapsed time: 7920.053201062 seconds (15272350808 bytes allocated, 4.07% gc time)
	dev3: elapsed time: 333.014117544 seconds (34072312 bytes allocated)
	tst3: elapsed time: 472.73413071 seconds (48172960 bytes allocated, 0.01% gc time)


	* test/zn11perc.pbs.o12463690: without shuffling, regular
	perceptron on the original zn11 feature set converges in 10 epochs
	to: (10,0.983861168363737,0.9627768956451571,0.9632195769145722)
	epoch,trn,dev,tst and stays there.

	* dbg/test/zn11perc.pbs.o12470963: zn11perc.jl: multi-seed
	shuffling version. The different seeds converge to slightly
	different solutions, but strangely each run converges and stops
	moving in exactly 10 epochs?  Other than that little funny
	coincidence the results are fairly consistent around:
	trn=9865 dev=9640 tst=9638

	These results are with Float32 and converge early.  Rerun with Float64.
	(seed,epoch,accuracy(ytrn,ztrn),accuracy(ydev,zdev),accuracy(ytst,ztst))
	(1,10,0.9866094775191278,0.9642606137907697,0.9638553106803273)
	(2,10,0.9866836373704125,0.9640784027904313,0.9637447482862829)
	(3,10,0.9865391629934651,0.9637139807897546,0.9638276700818161)
	(4,10,0.9865957442133343,0.9639742822188094,0.9639198054101865)
	(5,10,0.9865688269339791,0.9642606137907697,0.9638276700818161)
	(6,10,0.9865523469670269,0.9641174480047896,0.9639013783445124)
	(7,10,0.98665122676874,0.9638181013613765,0.9636526129579126)
	(8,10,0.986530923009989,0.964091417861884,0.9638553106803273)
	(9,10,0.9865122457141099,0.9636489054324908,0.9638276700818161)
	(10,10,0.9865122457141099,0.9638441315042819,0.9638368836146533)


2015-06-19    <dyuret@ku.edu.tr>

	* zn11train.jl:  Using perceptron.jl training with zn11orig xtst
	takes 500-600s (~100wps) and testing with xdev+xtst takes
	130-140s (~733wps).  At this speed training with xtrn would take
	9500s/epoch (158 min).  We can get 20 epochs in 52 hours.
	:(trn02-21:39832s,950028w,1820392m;dev22:1700s,40117w,76834m;tst23:2416s,56684w,108536m)

	Keeping w full instead of sparse may speed things up.  Also remove
	transpose and implement special purpose addx! function for update.
	New speed: 27.9s for xtrn (34051 wps).

	* zn11train2.jl: Using kperceptron.jl training with zn11single
	data.  100 iter x 128 batch training takes 15.33s (417 wps) for
	epoch1, 47.42s for epoch2.  Testing with dev+tst takes
	270.85s (357 wps).

	* TODO: Keeping K full instead of sparse may speed things up.
	:(Correction: K is already full.  It becomes full after the +p[1]
	operation).  Writing a more efficient Ac_mul_B may help.  If we do
	that we could keep sv as an array of vectors rather than a matrix.
	:(A sequence of dot products is 4 times slower than the fast mul
	implemented in julia, w product is negligible, the whole cost is
	in kpoly). Otherwise may have to do GPU.

	* DONE: zn11train.jl: running 100 epochs, try shuffling next.  what
	is the unlabeled move accuracy?  what is the parsing accuracy?
	Result is monotonically increasing, here is trn/dev/tst at epoch100:
	:(100,0.8464457105942017,0.8366738683395372,0.8390211542713938)

	* TODO: @@zn11train.jl: with shuffling this time: shuffling takes
	too long.  Do implicit shuffling by having an array of indices
	shuffled (randperm?)  But that would mess with the x2b copy
	procedure.

	* TODO: test comparing kperceptron with linear kernel and perceptron.

	* DONE: very large height impractical when taking transpose of w in perceptron.
	- cap the size in oparse.
	- investigate transpose vs no transpose.
	- implement mmul in terms of dot products. (but then regular arrays? override if sparse arrays? what is Ac_mul_B doing anyway?)

	* DONE: rewrite kernel perceptron.

2015-06-16    <dyuret@ku.edu.tr>

	* DONE: use Daume's trick for averaging?

	* DONE: kernel.jl currently uses averaged weights for both
	training and testing?  we need a training and testing mode?  why
	can't we use averaged weights during training as well?

2015-06-12    <dyuret@ku.edu.tr>

	* DONE: compare sfeatures vs dfeatures using the same feature set
	and algorithm to make sure they give the same result?  Or extract
	features on a short sentence using oparser?

	* DONE: implement averaging in perceptron.

	* DONE: implement ZN11 features (both simple and compound) in
	flist.

	* TODO: extract features using oparser and run compound feature
	and perceptron vs simple feature and kernel perceptron
	experiments.

	* acl11hybrid: dictionary.

		paper		GN13		acl11		label
		:----		:---		:----		:----
	single	S0w		s0w		STw		StackWord
		S0p		s0p		STt		StackTag
		S0wp		s0wp		STwt		StackWordTag
		N0w		n0w		N0w		NextWord
		N0p		n0p		N0t		NextTag
		N0wp		n0wp		N0wt		NextWordTag
		N1w		n1w		N1w		Next+1Word
		N1p		n1p		N1t		Next+1Tag
		N1wp		n1wp		N1wt		Next+1WordTag
		N2w		n2w		N2w		Next+2Word
		N2p		n2p		N2t		Next+2Tag
		N2wp		n2wp		N2wt		Next+2WordTag
	unigram	S0hw		s0hw		STHw		StackHeadWord
		S0hp		s0hp		STHt		StackHeadTag
		S0L		s0L		STi		StackLabel
	third	S0h2w		s0h2w		STHHw		StackHeadHeadWord
		S0h2p		s0h2p		STHHt		StackHeadHeadTag
		S0hL		s0hL		STHi		StackLabel ?? StackHeadLabel
	unigram	S0lw		s0lw		STLDw		StackLDWord
		S0lp		s0lp		STLDt		StackLDTag
		S0lL		s0lL		STLDi		StackLDLabel
		S0rw		s0rw		STRDw		StackRDWord
		S0rp		s0rp		STRDt		StackRDTag
		S0rL		s0rL		STRDi		StackRDLabel
		N0lw		n0lw		N0LDw		NextLDWord
		N0lp		n0lp		N0LDt		NextLDTag
		N0lL		n0lL		N0LDi		NextLDLabel
	third	S0l2w		s0l2w		STL2Dw		StackL2DWord
		S0l2p		s0l2p		STL2Dt		StackL2DTag
		S0l2L		s0l2L		STL2Di		StackL2DLabel
		S0r2w		s0r2w		STR2Dw		StackR2DWord
		S0r2p		s0r2p		STR2Dt		StackR2DTag
		S0r2L		s0r2L		STR2Di		StackR2DLabel
		N0l2w		n0l2w		N0L2Dw		NextL2DWord
		N0l2p		n0l2p		N0L2Dt		NextL2DTag
		N0l2L		n0l2L		N0L2Di		NextL2DLabel
	unused					HTw		HeadStackWord
						HTt		HeadStackTag
						HTwt		HeadStackWordTag
	pair	S0wpN0wp	s0wp,n0wp	STwtN0wt	StackWordTagNextWordTag
		S0wpN0w		s0wp,n0w	STwtN0w		StackWordTagNextWord
		S0wN0wp		s0w,n0wp	STwN0wt		StackWordNextWordTag
		S0pN0wp		s0p,n0wp	STtN0wt		StackTagNextWordTag
		S0wpN0p		s0wp,n0p	STwtN0t		StackWordTagNextTag
		S0wN0w		s0w,n0w		STwN0w		StackWordNextWord
		S0pN0p		s0p,n0p		STtN0t		StackTagNextTag
		N0pN1p		n0p,n1p		N0tN1t		NextTagNext+1Tag
	three	N0pN1pN2p	n0p,n1p,n2p	N0tN1tN2t	NextTagTrigram
		S0pN0pN1p	s0p,n0p,n1p	STtN0tN1t	StackTagNextTagNext+1Tag
		S0pN0pN0lp	s0p,n0p,n0lp	STtN0tN0LDt	StackTagNextTagNextLDTag
	third	N0pN0lpN0l2p	n0p,n0lp,n0l2p	N0tN0LDtN0L2Dt	StackTagNextTagNextLDTagNextTagNextL2DTag ?? NextTagNextLDTagNextL2DTag
		S0hpS0pN0p	s0hp,s0p,n0p	STHtSTtN0t	StackHeadTagStackTagNextTag
	unused					HTtHT2tN0t	HeadStackTagHeadStack2TagNextTag
	third	S0pS0hpS0h2p	s0p,s0hp,s0h2p	STHHtSTHtSTt	StackHeadHeadTagStackHeadTagStackTag
	three	S0pS0lpN0p	s0p,s0lp,n0p	STtSTLDtN0t	StackTagStackLDTagNextTag
	third	S0pS0lpS0l2p	s0p,s0lp,s0l2p	STtSTLDtSTL2Dt	StackTagStackLDTagStackL2DTag
	three	S0pS0rpN0p	s0p,s0rp,n0p	STtSTRDtN0t	StackTagStackRDTagNextTag
	third	S0pS0rpS0r2p	s0p,s0rp,s0r2p	STtSTRDtSTR2Dt	StackTagStackRDTagStackR2DTag
	dist	S0wd		s0wd		STwd		StackWordDist
		S0pd		s0pd		STtd		StackTagDist
		N0wd		n0wd		N0wd		NextWordDist
		N0pd		n0pd		N0td		NextTagDist
		S0wN0wd		s0w,n0w,d	STwN0wd		StackWordNextWordDist
		S0pN0pd		s0p,n0p,d	STtN0td		StackTagNextTagDist
	valency	S0wvr		s0wvr		STwra		StackWordRightArity
		S0pvr		s0pvr		STtra		StackTagRightArity
		S0wvl		s0wvl		STwla		StackWordLeftArity
		S0pvl		s0pvl		STtla		StackTagLeftArity
		N0wvl		n0wvl		N0wla		NextWordRightArity ?? NextWordLeftArity
		N0pvl		n0pvl		N0tla		NextTagRightArity  ?? NextTagLeftArity
	labset	S0wsr		s0wsr		STwrp		StackWordRightSetoftags
		S0psr		s0psr		STtrp		StackTagRightSetoftags
		S0wsl		s0wsl		STwlp		StackWordLeftSetoftags
		S0psl		s0psl		STtlp		StackTagLeftSetoftags
		N0wsl		n0wsl		N0wlp		Next0WordLeftSetoftags ?? NextWordLeftSetoftags
		N0psl		n0psl		N0tlp		Next0TagLeftSetoftags  ?? NextTagLeftSetoftags
	unused					STl		StackLemma
						STc		StackCPOS
						STf		StackFeats
						N0l		NextLemma
						N0c		NextCPOS
						N0f		NextFeats
						N1l		Next+1Lemma
						N1c		Next+1CPOS
						N1f		Next+1Feats

2015-06-11    <dyuret@ku.edu.tr>

	* SPARSE-KUPARSER:
	- Debug fix kuparser with new kunet.
	- Sparse features in kuparser: create a new branch.
	-- a feature vector is an array of feature elements
	-- a feature element is an array of primitive features
	-- we know the size for each primitive feature so we can get the sizes for elements and vectors.
	- features are always binary; i.e. a feature vector can be seen as a binary vector or a list of integers (for nonzero positions)
	- unfortunately bool matrices have no mmul, but sparse bool matrices do
	- we can do Int8, or just keep Float32.
	- there is also a bitarray
	- sparse.colptr always has number of columns entries, this limits the number of instances.
	- rowval and nzval have number of nonzero entries, this means xrows can be up to 1<<64.
	;;; So let's use sparse matrices and the rest of kuparser, kunet.
	; we just need to rewrite features.jl to support sparse / compound features.
	; we need dictionaries of words and pos: available via corpus
	; currently corpus has deprel and postag dictionaries.
	; in the sentences head, deprel, postag are Uint8 arrays, form is
	UTF8String array.
	; deprel, postag are closed class, but what to do with unknown
	words in test set.  Use a closed vocabulary with unk.
	; acl11.trn.wvec has 44389 unique words, 20162 used more than
	once.  decide on an unknown word policy.  what do zhang and
	goldberg do?  it turns out nothing.  unknown words just have zero weights in the perceptron.
	; acl11 uses perceptron training!  so I should be able to
	replicate exactly!  what kind of perceptron?  seems to be weighted perceptron.
	??? acl11/src/english/dependency/rules/penn.h has a bunch of
	constraints on which labels can link to each other etc.  cheating?
	should look carefully at this code...

	; figure out the vocab (fixed,open,encoding?), features (numeric
	encoding,can we spit out the actual feature vectors?), and
	perceptron details (averaged, structured?) and replicate acl11
	first (would require beam to work with sparse).  then use kernel
	perceptron to compare.  In fact if we could spit out the features
	we could do a simple static test of kernels (not beam).

	; run.en.log in acl11... started 1:45AM Jun 11 parcore-6-0.
	; 1 epoch trn takes 520 secs, dev takes 39, tst takes 53.
	; what are the 3 numbers printed out by eval.py? UAS, LAS, UEM
	; was able to confirm the results in the paper dev=9314 tst=9290 running run.en.sh in acl11.
	? does this not use the gold postags? devi.txt and tsti.txt could have generated postags. check.
	; data/en/model might be readable

	* ACL11-MODEL: probably we can answer most questions about the
	acl11 results using the model file data/en/model:

	Text file with dependency labels on the first two lines followed by
	blocks for each feature separated by empty lines.
	Ending with a Rules={0,1} line.
	Each block starts with a line with the feature name and a count.
	? Is the count the number of unique values?
	Each subsequent line in block has a value, colon, list of weights in {}.
	Each weight has (a : b / c)
	? a is the move
	? b is the perceptron weight
	? c is the summed perceptron weight for averaged perceptron

	;; maybe we leave the unknowns alone, the perceptron simply pretends they have zero weight.
	;; StackWord can be less than the actual words due to early stopping?  pruning?

	* ACL11-MODEL-BLOCKS: perl -ne 'print if $prev !~/\S/; $prev=$_;' data/en/model
	Q: what is the deal with the word counts?  Seems to be upper bound.
	Q: StackLabel duplicated, is it really a different feature?
	Q: how are the penn rules used?
	Q: what is the deal with the moves / weights?

	Dependency labels:	;; not a real block
	StackWord 31308		;; [In] unique words? 29874 entries?
	StackTag 45		;; DT  really has 45 entries.
	StackWordTag 37111	;; moment/NN  unique word-tag pairs? word/POS format
	NextWord 28572		;; [In] 27040 actual
	NextTag 45
	NextWordTag 33317
	Next+1Word 26850
	Next+1Tag 45
	Next+1WordTag 31120
	Next+2Word 28530
	Next+2Tag 45
	Next+2WordTag 32602
	StackHeadWord 20415
	StackHeadTag 45
	StackLabel 12		;; 10  represented by numbers 1-12
	StackHeadHeadWord 17491
	StackHeadHeadTag 45
	StackLabel 12		;; duplicate?? entries different!
	StackLDWord 22188
	StackLDTag 45
	StackLDLabel 12
	StackRDWord 20933
	StackRDTag 44		;; one tag missing?  44 actual.
	StackRDLabel 12
	NextLDWord 23384
	NextLDTag 45
	NextLDLabel 12
	StackL2DWord 13754
	StackL2DTag 44		;; one tag missing?
	StackL2DLabel 9		;; three labels missing?
	StackR2DWord 13535
	StackR2DTag 44
	StackR2DLabel 11
	NextL2DWord 17555
	NextL2DTag 45
	NextL2DLabel 12
	HeadStackWord		;; HeadStack not used
	HeadStackTag
	HeadStackWordTag
	StackWordTagNextWordTag 476465	;; books/NNS , estimates/NNS  bigrams?
	StackWordTagNextWord 467993	;; [push] , [for] , NN
	StackWordNextWordTag 461620	;; [became] , [would] , MD
	StackTagNextWordTag 158043	;; [subsidizing] , VB , VBG
	StackWordTagNextTag 176869	;; [TIGRs] , NNP , NN
	StackWordNextWord 452725	;; [colors] , [times]
	StackTagNextTag 1627		;; [ $ CC ]  45*45=2025
	NextTagNext+1Tag 1302		;; [ $ CD ]
	NextTagTrigram 12851		;; [ RBR `` JJ ] 45*45*45=91125
	StackTagNextTagNext+1Tag 21029	;; tagtrigram: uses special symbol -NONE-
	StackTagNextTagNextLDTag 13981	;; tagtrigram
	StackTagNextTagNextLDTagNextTagNextL2DTag 7439	;; tagtrigrams from here on
	StackHeadTagStackTagNextTag 19698
	HeadStackTagHeadStack2TagNextTag	;; not used.
	StackHeadHeadTagStackHeadTagStackTag 10904
	StackTagStackLDTagNextTag 15297
	StackTagStackLDTagStackL2DTag 6342
	StackTagStackRDTagNextTag 14382
	StackTagStackRDTagStackR2DTag 8998
	StackWordDist 94347	;; [nature] , 4  ;; 1-6, 5:[5-10], 6:[11+]
	StackTagDist 269	;; Dist does not appear by itself
	NextWordDist 82991
	NextTagDist 261
	StackWordNextWordDist 561392
	StackTagNextTagDist 6708
	StackWordRightArity 75076	;; [crabby] , 0
	StackTagRightArity 337		;; 0-21, 51-53 arity unfiltered
	StackWordLeftArity 73590
	StackTagLeftArity 353
	NextWordRightArity 83417
	NextTagRightArity 422
	StackWordRightSetoftags 84925	;; [forever] , [ AMOD P ]
	StackTagRightSetoftags 1434	;; Setoftags alphabetically sorted and uniqued
	StackWordLeftSetoftags 68065	;; 206 uniq setoftags
	StackTagLeftSetoftags 920
	Next0WordLeftSetoftags 73823
	Next0TagLeftSetoftags 1058
	StackLemma		;; Lemma, CPOS, Feats not used
	StackCPOS
	StackFeats
	NextLemma
	NextCPOS
	NextFeats
	Next+1Lemma
	Next+1CPOS
	Next+1Feats
	Rules=0			;; Don't know what this is, but it is an empty block

	* ACL11-SRC:
	./src/common/parser/main.cpp
	;; args: input, output, model
	;; main -> process -> parser.parse( input_sent , outout_sent , nBest , scores )

	./src/common/parser/train.cpp
	;; args: data, model, iters
	;; main -> auto_train(options.args[1], options.args[2], bRules, sSuperPath, bCoNLL);
	;; -> parser.train( ref_sent, nCount );
	;; -> parser.finishtraining();

	./src/common/parser/implementations/arceager/depparser.cpp: %.h, depparser_weight.h
	;; 716:parse -> work(false, sentence, retval, empty, nBest, scores )
	;; 738:train -> work( true , sentence , &outout , correct , 1 , 0 )
	;; 513:work
	;; -> bool bContradictsRules?
	;; set to true if:
	;;         if ( correct[index].head == DEPENDENCY_LINK_NO_HEAD && canBeRoot(m_lCache[index].tag.code())==false) {
	;;         if ( correct[index].head < index && hasLeftHead(m_lCache[index].tag.code())==false) {
	;;         if ( correct[index].head > index && hasRightHead(m_lCache[index].tag.code())==false) {
	;;         if (m_weights->rules() && !canAssignLabel(m_lCache, correct[index].head, index, m_lCacheLabel[index])) {
	;; so we use canBeRoot, hasLeftHead, hasRightHead, canAssignLabel built-in?
	;; do we have supertags?
	;; 539:      if (bTrain && m_weights->rules()) { does a bunch of rule checking during training and skips stuff
	;; 585:skip training example if it contradicts rules (but not testing?)
	;; 613:moves: reduce, poproot at the end. shift, arcleft, arcright.
	;; 620:keep only one global root?
	;; 618:move preconditions here, uses canBeRoot, canShift, canBeRoot, hasRightHead
	;; 628:canArcRight, hasLeftHead
	;; we have conditions based on supertags and rules! (is it one or the other?)
	;; 642:canArcLeft, hasRightHead
	;; 667:EARLY_UPDATE
	;; 675:adding labels and making move
	;; does not seem to look at rules.  m_weights->rules()=0.  RULES=0 at the end of model file?
	;; m_supertags is 0 as well. so ignore all that code.
	;; foo.model: single sentence training, supertags and rules both 0.
	;; error at 4th word out of 49 (early stop, only the first 3 words enter model)
	;; foo2.model: single sentence starting from date/en/model.
	;; the third number is definitely the sum for averaged perceptron
	;; 0 error so initial numbers did not change, but the sums did.
	;; for all features, not just the sentences parsed.
	;; sum per epoch or sum per sentence? -- per sentence
	;; the new models have the counts correct as well
	;; maybe the other model had it wrong because of not taking the last epoch?
	;; TODO: can we have it spit out (oracle) features?
	;; DONE: is he keeping weights as integers?  64bit?
	;; scores read in score_packed_list.h:41?  does not handle / sum.
	;; defined in score.h:45, fields SCORE_TYPE current and total.
	;; depparser_macros.h defines SCORE_TYPE as long, i.e. Int32
	;; it also has encode_link_distance:
	if (diff>10) diff = 6;
	else if (diff>5) diff = 5;
	return diff;
	;; TODO: we need to make sure right features, feature combos (beware of duplicate) and move set move conditions.
	;;; loading scores: find out what happens to duplicate featurename: done, a number of misspellings
	;;; TODO: parsing: see if we can output the features as the parser moves
	;;;; TODO: which instances does acl11 train on?
	;;;; DONE: what beam size? AGENDA_SIZE=64 in depparser_macros.h

	./src/common/parser/implementations/arceager/depparser_weight.cpp: %.h
	./src/libs/linguistics/lemma.cpp: %.h
	./src/libs/linguistics/conll.cpp: %.h
	./src/libs/options.cpp: %.h
	./src/libs/reader.cpp: %.h
	./src/libs/writer.cpp: %.h

	;; headers
	./src/include/definitions.h: sentence_string.h
	./src/include/linguistics/sentence_string.h: utf.h
	./src/include/linguistics/lemma.h: tokenizer.h
	./src/include/options.h: definitions.h
	./src/include/reader.h: definitions.h, file_utils.h, sentence_string.h
	./src/include/writer.h: definitions.h, sentence_string.h
	./src/include/file_utils.h:
	./src/include/utf.h:

	./src/include/linguistics/conll.h: utils.h dependency.h dependencylabel.h generictag.h
	./src/include/linguistics/dependencylabel.h: dependency.h
	./src/include/linguistics/dependency.h: sentence_template, dependency_utils
	./src/include/linguistics/generictag.h: tokenizer
	./src/include/linguistics/dependency_utils.h: sentence_string
	./src/include/linguistics/sentence_template.h:
	./src/include/utils.h: definitions.h

	./src/common/parser/implementations/arceager/depparser.h: depparser_base.h, state.h, depparser_weight.h
	./src/common/parser/implementations/arceager/depparser_weight.h: depparser_weight_base.h
	./src/common/parser/implementations/arceager/state.h:
	./src/common/parser/depparser_base.h: depparser_base_include.h, depparser_weight_base.h
	./src/common/parser/depparser_weight_base.h: depparser_base_include.h

	./src/common/parser/depparser_base_include.h: definitions, utils, tags, word_tokenized, taggedword, dependency, dependencylabel, dep, depparser_macros, conll, depparser_impl_inc, tagset, options, supertag
	./src/english/tags.h: pos/penn.h pos/penn_morph.h
	./src/english/dep.h: dependency/label/penn.h dependency/rules/penn.h
	./src/include/linguistics/word_tokenized.h: tokenizer, bigram, word_common
	./src/include/linguistics/taggedword.h:
	./src/include/linguistics/tagset.h:
	./src/common/parser/implementations/arceager/depparser_macros.h: action.h
	./src/common/parser/supertag.h:
	./src/english/pos/penn.h:
	./src/english/pos/penn_morph.h: tagdict.h
	./src/include/knowledge/tagdict.h: hash.h
	./src/english/dependency/label/penn.h: tags
	./src/english/dependency/rules/penn.h: tags, dependency/label/penn.h
	./src/include/bigram.h:
	./src/include/linguistics/word_common.h:
	./src/common/parser/implementations/arceager/action.h:

	./src/common/parser/implementations/arceager/depparser_impl_inc.h: agenda, pair_stream, hashmap_score_packed, bigram, tuple{2,3,4}, lemma, charcat
	./src/include/agenda.h: definitions
	./src/include/pair_stream.h:
	./src/english/charcat.h:
	./src/include/tuple4.h:
	./src/include/tuple3.h:
	./src/include/tuple2.h:

	./src/include/learning/perceptron/hashmap_score_packed.h: hash, hash_small, score, score_packed_list
	./src/include/hash_small.h: pool.h
	./src/include/learning/perceptron/score_packed_list.h: hash, score, linkedlist
	./src/include/learning/perceptron/score.h:
	./src/include/linkedlist.h: pool.h

	./src/include/tokenizer.h: hash.h
	./src/include/hash.h: hash_utils, hash_simple, hash_stream
	./src/include/hash_utils.h: definitions
	./src/include/hash_simple.h: pool.h
	./src/include/hash_stream.h:
	./src/include/pool.h:

	;; unused?
	./src/libs/linguistics/cfg/generic.cpp
	./src/libs/linguistics/constituent.cpp
	./src/common/parser/test.cpp

	;; chinese specific
	./src/chinese/doc2snt/main.cpp
	./src/chinese/doc2snt/doc2snt.cpp

	;; labeler, when does it get used? run.en.sh uses data/en/*.txt files which have gold tags?
	./src/common/deplabeler/main.cpp
	./src/common/deplabeler/train.cpp
	./src/common/deplabeler/implementations/naive/deplabeler.cpp
	./src/common/deplabeler/implementations/naive/weight.cpp

2015-06-10    <dyuret@ku.edu.tr>

	* TODO: copy :gpu and :cpu does not work any more.  Supposed to
	use KUnet.atype before copy?  But we need to rethink atype in
	light of sparse arrays.  Until then multi-core stuff fails in
	KUparser.

	* TODO: add one-hot word and pos to features.jl.  test it with
	sparse arrays.  run the first half of the kernel vs linear
	experiment.
	; or extract strings and run kunet separately, easier debug?
	Figure out how to extract the combo features.
	; write special code just for this?
	Use
	; strings and liblinear?
	; Or perceptron with sparse features.
	;; but w will be full, is the dimensionality enough?


2015-05-05    <dyuret@ku.edu.tr>

	* paper: ideas:
	1. word vectors instead of words
	- basic approaches for word vectors
	- dense representations represent similarity (scode sphere)
	- dense representations allow nonlinear models
	- empirical comparison of word vector types on parsing
	- works with any word vector (but do need to construct context vectors)
	2. context vectors instead of pos:
	- history of pos, map vs territory
	- inconsistencies in pos (examples from mali paper)
	- empirical comparison of pos vectors vs context vectors (vs both vs neither)
	3. nonlinear models instead of linear+ad-hoc-feature-combos
	- examples of feature combos
	- kernel perceptron: memory problems
	- neural network

2015-05-02    <dyuret@ku.edu.tr>

	* vcirik: Selam,

	repository'e (https://github.com/wolet/conll2eparse) pos tagleri
	one-hot vector olarak ekleyen
	script'i (https://github.com/wolet/conll2eparse/blob/master/src/scripts/enrich_with_pos.py)
	ekledim. Var olan bir eParse corpusuna pos tag column'lari
	ekliyor.

	wikipedia100 icin,  /ai/home/vcirik/temp/conll2eparse/run altinda
	su datalari cikardim :

	type : conll-ptb_type_wikipedia2MUNK-100
	type + pos : conll-ptb_type_wikipedia2MUNK-100+pos
	type + context vec :
	conll-ptb_token_wikipedia2MUNK-100+wikipedia2MUNK-100
	type + context vec + pos :
	conll-ptb_token_wikipedia2MUNK-100+wikipedia2MUNK-100+pos

	Dikkat edilecek tek husus, bizim parser'i calistirirken embedding
	sayisina 45 eklemek lazim.

	kolay gelsin,
	volkan

	- comparing with original acl11 data: dev and tst are identical.
	There are pos differences in trn.  acl11 has NNP for some tokens
	that have different pos in ptb3/ptb2 (e.g. Revitalized Classics in
	the beginning of sec02).


2015-04-28    <dyuret@ku.edu.tr>

	* Summary: corpus-parser-arctype-feats where:
	corpus in acl11, con07
	parser in beam, dynamic, static
	arctype in hybrid, eager
	feats in ZN11e (arc11eager), GN13h (tacl13hybrid), or other

	acl11-beam64-eager-ZN11e: uas1:no uas2:no
	:(all except ZN11 scores include punct which approx -1.0%)
	uas2dev	uas2tst	exp	las2dev	las2tst	uem2dev	uem2tst
	9314	9290	ZN11	x	9180	5012	4800
	9212	9165	03290018-bparse64.out(113/166)
	9211	9171	03220927-bparselatestop.out(129/207)
	9205	9166	03220548-bparsedbg.out(193/256)
	9204	9189	04161249-bp64a11h25.out(65/130) (feats=hybrid25,uas2dev=9305,uas2tst=9270,model=04161131-bp64a11h25.jld)
	9186	9136	03201925-julia3_btrain_jl.out(77/77)
	9174	9182	04110746-bp64a11h27.out(45/59) (feats=hybrid27,got stuck)
	9173	9170	04162129-bp64a11e25.out(60/120) (feats=eager25,model=04160207-bp64a11e25.jld)
	9171	9152	03292013-bp10acl11eager.out(55/115)
	9167	9153	04022337-bp64a11eR1.out(55/110) trn:7m30s prs:96m epoch:27m (5t1p)
	9159	9142	03300736-bp10acl11eager13.out(75/155)
	9158	9145	03240436-bpacl11.out(79/256)
	9158	9140	03192351-gparsedbg.out(194/256)
	9156	9141	04030908-bp64a11e13.out(65/130) trn:7m30s prs:96m epoch:27m (5t1p)
	9154	9161	04110144-bp64a11e20.out(88/113) (feats=eager20,got stuck)
	9154	9129	03171146-julia3_gtrain_jl.out(93/127)
	9136	9109	03212217-gpacl11.out(184/256)
	9116	9074	03200328-oparsedbg.out(254/256)
	9099	9075	03160543-julia3_otrain_jl.out(128/256)
	9093	9060	03212217-opacl11.out(210/256)

	con07-dynamic-hybrid-GN13h: uas1:ok uas2:ok
	:(no averaging or dev set in my results except ones marked with +)
	8762-8840@5003 one-tailed p-value:0.1149
	uas	las	exp
	8762	8628	GN13
	8913	?	04170445-gp07h28b.out(98/196) (feats=hybrid28b)
	8903	x	04061328-gp07h14.out(314/458) (feats=hybrid14)
	8847	?	04161022-gp07h25.out(91/182) (feats=hybrid25)
	8840+	x	gp07h28b-?: (conll07 no dev) minlen=200, @050=8818, @100=8844, maxavg@113=8866, @150=8848, @200=8840
	8832+	x	gp07h28b-4?: (dev1000) 8855@122 8839@211 8851@123 8823@56 8793@89
	8829+	x	gp07h14-?: (conll07 no dev) minlen=238, @050=8782, @100=8792, maxavg@109=8774, @150=8825, @200=8829, maxavg@225=8845
	8827	x	03291208-gpconll07hybrid13.out(93/187)
	8811	x	04010424-gp07h13.out(164/328) (repeat of 03291208)
	8801	x	03292304-gpconll07hybrid3.out(226/453) (ArcHybridR1)
	8799	x	04010425-gp07h13d25.out(139/256) (dropout 0.2 0.5)
	8793	x	03210908-gpconll07.out(133/256) (fv022b, no deprel)
	8793	x	03311930-gp07h13k10.out(134/268) (10k hidden unit)
	8789	x	03270848-gptacl13wp.out(241/256) (all wp features)
	8789	x	03250401-gptacl13.out(169/213)
	8773	x	03271009-gptacl13.out(233/256)
	8766+	x	gp07h13-6?: (conll07 dev22) tst@maxdev: 8739 8753 8797 8733 8807 (>200 epochs)
	8764++	x	gp07h13-4?: (conll07 dev1000) tst@maxdev: 8799,8763,8737,8761,8761 (250-300 epochs)
	8761	x	04010123-gp07h13k44.out(138/276) (4k+4k hidden layers)
	8759+	x	04280722-gp07h13-n3.out: (3net experiment, conll07 no dev) len=408, @050=8715, @100=8733, @150=8745, @200=8759, maxavg@204=8837, @250=8767, @300=8757, @350=8753, @400=8765
	8756+	x	gp07h13-5?: (conll07 dev500) tst@maxdev: 8759 8727 8767 8755 8773
	8744+	x	gp07h13-2?: (conll07 no dev) minlen=208, @050=8726, @100=8739, maxavg@109=8774, @150=8734, @200=8744
	8737+	x	gp07h13-?: (conll07 dev1989 sec02) 8719 8721 8757 8733 8753 minlen=228, @050=8698, @100=8731, avgtst@maxavgdev@140=8744, @150=8748, @200=8764, maxavgtst@223=8775, tst@maxdev=8737
	8727	x	03260538-gptacl13nl.out(228/256) (no deprel)
	8717	x	03311154-gp07h13d25.out(16/32) (dropout 0.2 0.5, too short?)
	8709	x	03260132-gptacl13w.out(256/256) (only w features)
	8697+	x	gp07h13-3?: (conll07 dev200) minlen=100, @050=8690, @100=8726, avgtst@maxavgdev@76=8713, maxavgtst@86=8745, tst@maxdev=8697
	8659	x	03261450-gptacl13nd.out(26/256) (no dropout)
	8005	x	03311801-gp07h13lr1.out(1/148) (learningRate=1.0)
	7877	x	03270146-gptacl13p.out(187/256) (only p features)

	con07-static-hybrid-GN13h: uas1:ok uas2:ok
	8643-8806@5003 p-value: 0.0073 (http://in-silico.net/tools/statistics/ztest two proportion z-test)
	8643-8724@5003 p-value: 0.1154
	uas	las	exp
	8643	8496	GN13
	8887	x	04070512-op07h14.out(377/754) (feats=hybrid14)
	8823	x	03300624-opconll07hybrid13.out(242/485)
	8806+	x	op07h28b-?: minlen=200, @050=8777, @100=8801, @150=8799, maxavg@177=8831, @200=8806 (conll07 no dev)
	8803+	x	op07h14-?: (conll07 no dev) minlen=242 @050=8754 @100=8783 @150=8785 maxavg@185=8832 @200=8803 @240=8808
	8783	x	03282213-opconll07hybrid.out(79/159)
	8774+	x	op07h28b-4?: (dev1000) tst@maxdev: 8727 8767 8789 8783 8807 (100-150 epochs)
	8728+	x	op07h13-4?: (dev1000) tst@maxdev: 8749 8703 8705 8739 8743 (151-1998 epochs)
	8724+	x	op07h13-?:  minlen=146, @050=8700, maxavg@075=8741, @100=8710, @150=8724, @200=8715 (conll07 no dev)
	8699	x	03210931-opconll07.out(170/256) (fv022b, no deprel)
	8689	x	03271008-optacl13.out(234/256)
	8675	x	03242000-optacl13.out(66/78)

	con07-beam10-hybrid-GN13h
	uas	las	exp
	8865	x	03221208-bpconll07.out(54/256)
	9041	?	04190719-bp64c07h28b.out(50/100) (try beam parser with best features)

	con07-dynamic-eager-ZN11e: uas1:no uas2:yes
	8743-8869@5003 p-value: 0.026
	uas	las	exp
	8869	8769	GN13
	8897	x	04091636-gp07e20.out(62/124) (feats=eager20)
	8887	?	04101642-gp07e21b.out(138/276) (feats=eager21b)
	8883	?	04110758-eager21b.out(76/152) (feats=eager21b,shuffle)
	8867	?	04111209-eager23.out(96/192) (feats=eager23,shuffle)
	8861	x	04100103-gp07e20b.out(108/216) (feats=eager20b)
	8847	x	04110739-gp07e21.out(437/599) (feats=eager21)
	8847	?	04110758-eager20.out(104/208) (feats=eager20,shuffle)
	8843	?	04021402-gp07e13k44.out(95/190) (feats=eager39,4+4k hidden)
	8839	?	04020644-gp07e13k10.out(39/100) (feats=eager39,10k hidden)
	8837	?	04111211-eager21.out(173/233) (feats=eager21, shuffle)
	8831	?	04110758-eager39.out(51/102) (feats=eager39, shuffle)
	8829	?	04110424-gp07e36.out(75/150) (feats=eager36)
	8828+	x	gp07e21b-?: minlen=200, @050=8818, maxavg@066=8852, @100=8818, @150=8828, @200=8794 (conll07 no dev)(only 4 completed)
	8823	?	04031246-gp07e13l01.out(190/380) (learningRate 0.1)
	8821	x	03292103-gpconll07eager13.out(93/187)
	8821	?	04110758-eager36.out(56/112) (feats=eager36,shuffle)
	8817	x	04062220-gp07e36.out(71/100) (feats=eager36)
	8811	?	04110758-eager20b.out(75/150) (feats=eager20b,shuffle)
	8807	?	04101459-gp07e23.out(86/172) (feats=eager23)
	8801	?	04111024-gp07e39.out(96/192) (feats=eager39)
	8799	x	03290106-gpconll07eager.out(22/45)
	8798+	x	gp07e20-4?: (dev1000) tst@maxdev: 8815 8777 8787 8785 8829 (epochs 44-110? rerun with minepochs)
	8795	?	04021040-gp07e13d25.out(32/100) (dropout 0.2 0.5)
	8793	x	04161530-gp07e25.out(137/274) (feats=eager25)
	8785	?	04021101-gp07e13.out(33/100) (repl 03292103)
	8763	x	03281848-gpconll07eager.out(32/65)
	8743+	x	gp07e39-?: minlen=200, @050=8722, @100=8743, maxavg@106=8765, @150=8733, @200=8742 (conll07 no dev)
	8690+	x	gp07e39-4?: (dev1000) tst@maxdev: 8745 8671 8651 8709 8677 (27-39 epochs? rerun with minepochs)

	con07-static-eager-ZN11e: uas1:ok uas2:no
	8610-8655@5003 p-value: .2562
	8610-8739@5003 p-value: .0285
	uas	las	exp
	8610	8493	GN13
	8739+	x	op07e21b-?: minlen=268, @050=8671, @100=8698, @150=8701, @200=8711, maxavg@250=8739 (conll07 no dev)
	8685	x	03281640-opconll07eager.out(21/43)
	8681	x	03300220-opconll07eager13.out(102/139)
	8675	x	04070709-op07e36.out(28/100) (feats=eager36)
	8655+	x	op07e39-?: minlen=200, @050=8604, maxavg@073=8664, @100=8655, @150=8621, @200=8620 (conll07 no dev)
	8649	x	03292324-opconll07eager.out(31/63)
	8629+	x	op07e39-4?.out: (dev1000) tst@maxdev: 8663 8625 8599 8651 8607 (epochs 34-299)

2015-04-24    <dyuret@ku.edu.tr>

	* TODO:
	+ debug julia memory leak
	+ complete and test 3net implementation
	+ run half size dev (sec02/2)

	* 5try:
	+ try large devset (dev2=sec02, first 50k words, 1989 sentences, trn2=rest, tst=5k words)
	+ avg=8737
	$ for i in 1 2 3 4 5; do psub-ilac -N gp07h13-$i "julia3 train.jl conll07.trn2.jld4 conll07.dev2.jld4 conll07.tst.jld4 --parser gparser --arctype ArcHybrid13 --feats hybrid13 --pbatch 500 --seed $i"; sleep 2; done
	8719 (170/340) 04171715-gp07h13-1.out
	8733 (204/408) 04180121-gp07h13-2.out
	8757 (171/342) 04172134-gp07h13-3.out
	8721 (114/228) 04171801-gp07h13-4.out
	8753 (154/308) 04181110-gp07h13-5.out
	+ try smaller devset (dev3: last 200 sentences, 5011 words, trn3: rest, 18377 sentences, 441562 words)
	+ avg=8697
	$ for i in 1 2 3 4 5; do psub-ilac -N gp07h13-3$i "julia3 train.jl conll07.trn3.jld4 conll07.dev3.jld4 conll07.tst.jld4 --parser gparser --arctype ArcHybrid13 --feats hybrid13 --pbatch 500 --seed $i"; sleep 2; done
	8663 (46/100) 04181444-gp07h13-31.out
	8741 (58/116) 04181544-gp07h13-32.out
	8711 (55/110) 04181556-gp07h13-33.out
	8685 (70/140) 04181953-gp07h13-34.out
	8687 (64/128) 04182242-gp07h13-35.out
	+ try fixed epoch (150)
	+ avg=8734
	$ for i in 1 2 3 4 5; do psub-ilac -N gp07h13-2$i "julia3 train.jl conll07.trn.jld4 conll07.tst.jld4 --parser gparser --arctype ArcHybrid13 --feats hybrid13 --pbatch 500 --seed $i"; sleep 2; done
	8747 	gp07h13-21.out
	8763 	gp07h13-22.out
	8733	gp07h13-23.out (completed in gp07h13-3.out)
	8707	gp07h13-24.out
	8723	gp07h13-25.out
	= try fixed epoch with other parser/feature combos:
	= try smaller dev set. (dev4: first 1000 sentences, 24358 words, trn4: rest 17577 sents, 422215 words)
	for i in 1 2 3 4 5; do psub-ilac -N gp07h13-4$i "julia3 train.jl conll07.trn4.jld4 conll07.dev4.jld4 conll07.tst.jld4 --parser gparser --arctype ArcHybrid13 --feats hybrid13 --pbatch 500 --epochs 100 --seed $i"; sleep 2; done
	= try smaller dev set. (dev5: first 500 sentences, 11803 words, trn5: rest 18077 sents, 434770 words)
	for i in 1 2 3 4 5; do psub-ilac -N gp07h13-5$i "julia3 train.jl conll07.trn5.jld4 conll07.dev5.jld4 conll07.tst.jld4 --parser gparser --arctype ArcHybrid13 --feats hybrid13 --pbatch 500 --epochs 100 --seed $i"; sleep 2; done
	= try separate dev set (dev:sec22: pennconverter gives %5 different deprels but we'll try)
	for i in 1 2 3 4 5; do psub-ilac -N gp07h13-6$i "julia3 train.jl conll07.trn.jld4 conll07.dev.jld4 conll07.tst.jld4 --parser gparser --arctype ArcHybrid13 --feats hybrid13 --pbatch 500 --epochs 100 --seed $i"; sleep 2; done
	= try hybrid14
	for i in 1 2 3 4 5; do psub-ilac -N gp07h14-$i "julia3 train.jl conll07.trn.jld4 conll07.tst.jld4 --parser gparser --arctype ArcHybrid13 --feats hybrid14 --pbatch 500 --epochs 200  --seed $i"; sleep 2; done
	for i in 1 2 3 4 5; do psub-ilac -N op07h14-$i "julia3 train.jl conll07.trn.jld4 conll07.tst.jld4 --parser oparser --arctype ArcHybrid13 --feats hybrid14 --pbatch 500 --epochs 200  --seed $i"; sleep 2; done

2015-04-20    <dyuret@ku.edu.tr>

	* conll07: the paper says english has 20 deprels.  eval07.pl
	includes punctuation.  uses unpaired z-test for comparison.

2015-04-17    <dyuret@ku.edu.tr>

	* mcnemar: is a paired test.
	+ conll07 tst has 5003 words.
	+ example: ref-uas=8762 (4384-619), gp07h28b=8913 (4459-544)
	+ we need the number of conflicting cases, i.e. individual answers, not just percentages.

2015-04-16    <dyuret@ku.edu.tr>

	* conll07.dev.jld4: split sec02 (first 1989 sentences) as dev, named the remaining training set conll07.trn2.jld4.

	* src/util.jl: Bugun yine sacma bir sey farkettim.  Evaluation sirasinda "punctuation excluded" diye herkes farkli bir yontem kullaniyor, ve bu yontemler arasindaki fark 0.3%'e cikabiliyor. Ornegin benim beam parser'in su anki dev uas sonuclari:

	0.9276 according to conll07
	0.9305 according to ZN11
	0.9294 according to KCC08

	Karsilastirmalarda dikkatli olun, karsilastirdiginiz paper ne kullaniyorsa onu kullanin.  kullandigim exact definition'lar:

	# Everybody means something else by "excluding punctuation":

	# The buggy conll07 eval.pl script looks for non-alpha characters, but excludes ` and $, also won't match -LRB-, -RRB- etc.
	c07punct(w)=(ismatch(r"^\W+$",w) && !ismatch(r"^[`$]+$",w))

	# ZN11 eval.py uses his own regexp for punctuation (I think both for chinese and english)
	zn11punct(w)=ismatch(r"^[,?!:;]$|^-LRB-$|^-RRB-$|^[.]+$|^[`]+$|^[']+$|^（$|^）$|^、$|^。$|^！$|^？$|^…$|^，$|^；$|^／$|^：$|^“$|^”$|^「$|^」$|^『$|^』$|^《$|^》$|^一一$",w)

	# KCC08 uses postags rather than wordforms, but for some reason skips #, $, -LRB-, -RRB- etc.
	kcc08punct(p)=in(p,["``", "''", ":", ",", "."])


	* TODO:
	- add sbatch option to train net every n sentences for bparser.
	- find the fastest way to run beam64.
	+ finish 3net: no significant difference.
	- debug realpos.
	-- why is realpos so bad?  is it that bad if we simply drop p features or is it a bug?
	- finish fselect: c07hybrid, c07eager, a11hybrid, a11eager.
	- figure out dev set vs fixed epoch for c07.
	= finish 5x experiments.
	- finish embedding experiments.
	- compare regexp based vs postag based word score.
	+ does it matter if bp pepochs=1 or 5? not really: bp64c07h28b*

	* TODO:
	- add sbatch option to train net every n sentences.

	- figure out eval.pl vs discounting punct pos for acl11.
	- figure out fixed epoch vs dev set for conll07.
	- should we not use bparser during fselect?

	- Leave postags as postags, compare with context vectors.

	- Try the split experiment with three networks (move, rlabel, llabel).

	- Explore training options.
	-- Try move+label nnet output?
	-- cost function of nnet, cost fn of parser?

	- Run five experiments and report avg-std.

	- Explore feature options.
	  training epoch timing: conll07-tacl13hybrid: 120s, conll07-acl11eager: 220s, acl11-acl11eager: 448s
	  tacl13hybrid has 13 features: [(s0,s1,n0,n1)(wp),(s1l,s1r,s0l,s0r,n0l)(p)]
	  acl11eager has 39: [n0(wpAa),s0(wpLdAaBb),(n1,n2,s0h2)(wp),(n0l2,n0l,s0h,s0l2,s0l,s0r2,s0r)(wpL)]
	  Nobody has heads in hybrid, si may have heads in eager.
	  In both si have children, but only n0l in buffer.
	  s1 is important in hybrid, so we should add its children.
	  we should subtract heads and labels for all si,ni for hybrid.  So we have 54:
	  hybrid54: [n0(wpAa),(s0,s1)(wpdAaBb),(n1,n2,s2)(wp),(n0l2,n0l,s0l2,s0l,s0r2,s0r,s1l2,s1l,s1r2,s1r)(wpL)]
	  our previous best feature sets:
	  fv021a: [n0(wpa),s0(wpb-),(s1,s0r)(wb),(n1,n0l)(wp),s2(w),s0l(wpb),(s1r,s2r)(a)]
	  fv022b: [n0(wpa),s0(wpabH),s1(wpbdH),(n1,n0l)(wp),(s0r,s0l)(w),(s1l)(pb),(s2)(W)]

	  For conll07/acl11 hybrid: start with tacl13hybrid or fv022b, do a search within hybrid54.
	  For conll07/acl11 eager : start with acl11eager and do a search within (maybe add s1?)
	  Each one level search means trying out 40-50 features.
	  Per epoch is 100min for 120s, 180min for 220s, 360min for 448s.
	  To finish one level in 24 hours we can do 14 epochs for 100min, 8 epochs for 180min, 4 epochs for 460 min.

	- try previous moves as features? (rnn idea)
	- try arceasy (better on Goldberg&Nivre13)
	- train with oparse, parse with bparse?

	- Write paper.

2015-04-13    <dyuret@ku.edu.tr>

	* test/fselect.jl: FEATURE SELECTION EXPERIMENTS

	corpus: c07, a11
	parser: ArcEager13, ArcHybrid13
	eager-all: eager39, eager64
	eager-start: eager39, eager36, eager21, eager20
	hybrid-all: hybrid54
	hybrid-start: hybrid13, hybrid14, hybrid28
	:rename acl11eager->eager39, tacl13hybrid->hybrid13.

	Results:

	fs07h54h28: h28=8683 -> h28b=8797 (04081831-fs07h54h28.cache)
	fs07h54h13: h13=8613 -> h14=8791 (04050753-hybrid54.cache)

	fs07e39e39: e39=8619 -> e36=8725 (04060625-fs07e39e39.cache)
	fs07e39e21: e21=8623 -> e20b=8705 (04072145-fs07e39e21.cache)
	fs07e64e20: e20=8613 -> e21b=8677 (04080348-fs07e64e20c.cache)

	fs11h54h27: h27=9111 -> h25=9144 (04132230-fs11h54h27.cache, bparser10, shuffle)
	fs11h54h14: h14=8931 -> h25b=8992 (04122017-fs11h54h14.cache)

	fs11e64e27: e27=8994 -> e25=9098 (04142002-fs11e64e27.cache, e27=h27 run with ArcEager13, bparser10, shuffle)
	fs11h54h28: h28=8978 -> h27=8997 (04071554-fs11h54h28.cache) BUG: this was run with ArcEager13!
	fs11e64e20: e20=8926 -> e20=8926 (04071209-fs11e64e20.cache)
	fs11e39e21: e21=8880 -> e23=8906 (04061344-a11e21.cache)
	fs11e39e39: e39=8906 -> e39=8906 (04042125-acl11eager2.cache)

2015-04-10    <dyuret@ku.edu.tr>

	* test/fselect.jl: fixed bug: tasks that end with error do not get
	added back on the queue.

	* realpos: Compare the performance of using real pos features
	rather than context vectors.

	8127	04110236-gp07h14rp.out(144/288)
	8893	04112133-gp07h14.out(470/559)

2015-04-09    <dyuret@ku.edu.tr>

	* shuffle: Added shuffling option to KUnet train.  Makes a big
	difference when sentences parsed in a big batch.  Otherwise all
	sentence initial parts go together in the beginning and hurt
	training.  Results prior to 2015-04-09 suspect.

2015-04-05    <dyuret@ku.edu.tr>


	* 04050753-hybrid54.cache: optimizing ArcHybrid13 features on conll07.

	tacl13hybrid	.8613	n0lp n0p      n0w n1p n1w     s0lp s0p s0rp s0w     s1lp s1rp s1p s1w
	hybrid14	.8791	n0lp     n0lw n0w n1p n1w s0d s0lp s0p s0rp s0w s1B s1lp      s1p s1w

	0.06356	s0w	0.01439	s0p	0.01119	+n0p
	0.06276	n1w	0.01279	s0rp	0.00080	+s1rp
	0.05737	n0w	0.01199	s0d*	
	0.02319	s1w	0.01199	n1p	
	0.01959	s1B*	0.00999	n0lw*	
	0.01659	n0lp	0.00979	s1p	
	0.01519	s0lp	0.00919	s1lp	

	* 04042125-acl11eager2.cache: unfortunately we cannot beat the
	original feature set with single step moves:

	acl11eager(39)	.8906
	best38		.8899

	But we do have a ranking of all 39 features by how much their
	removal effects performance:

	0.03849	n0w	0.00394	s0L	0.00307	n2w	0.00232	s0rw	
	0.02894	s0w	0.00394	s0B	0.00284	s0h2w	0.00197	n0l2w	
	0.02445	n1w	0.00364	n0lL	0.00282	s0lL	0.00162	s0rp	
	0.00678	s0p	0.00359	s0r2p	0.00274	s0rL	0.00160	n0lp	
	0.00481	s0l2p	0.00359	s0hw	0.00257	s0hp	0.00157	s0hL	
	0.00439	n1p	0.00359	n0A	0.00252	n0l2L	0.00112	s0r2w	
	0.00436	s0h2p	0.00354	s0lp	0.00244	s0A	0.00080	n0l2p	
	0.00416	n2p	0.00327	s0r2L	0.00237	s0d	0.00072	s0lw	
	0.00409	s0l2L	0.00327	s0l2w	0.00237	n0a	0.00067	n0lw	
	0.00401	n0p	0.00317	s0a	0.00234	s0b	

	+ do we have overfitting because of no dropout (s0l2p?, s0h2p?)
	+ should we try features of s1?

	* 04050905-acl11eager.cache: stopped early, same result as acl11eager2.


2015-04-03    <dyuret@ku.edu.tr>

	* fselect: feature selection.  up to ncpu=6 and pbatch=500 can be
	supported on ilac with acl11eager (700MB per cpu).  TODO: feature
	set for archybrid.

 480368 0.50531 hybrid54   dyuret       r     04/03/2015 23:02:25 ilac.q@ilac-0-3.local              1
 480370 0.50531 acl11eager dyuret       qw    04/03/2015 23:04:42                                    1
   3969 0.60500 acl11eager2 dyuret       r     04/03/2015 23:07:22 iui.q@iui-5-0.local                1

2015-04-02    <dyuret@ku.edu.tr>

	* Latex-table:

	ref	parser		feats		corpus	ref-uas		ku-uas1		ku-uas2
	GN13	hybrid:static	hybrid13	conll07	.8643		.8823		?
	GN13	hybrid:dynamic	hybrid13	conll07	.8762		.8827		?
	GN13	eager:static	eager39		conll07	.8610		.8685		?
	GN13	eager:dynamic	eager39		conll07	.8869		.8821		?
	ZN11	eager:beam	eager39		ptb12	.9290		.9225		?
	MCP05	mst1?		?		?	.9090		?		?
	MP06	mst2,proj2?	?		?	.9150		?		?
	E96	proj1/2?	?		?	?		?		?
	C07	proj2+?		?		conll07	.9063		?		?
	KC10	order3?		?		?	.9304		?		?
	KCC08	semisup08?	?		?	.9316		?		?
	SICC09	semisup09?	?		?	.9379		?		?
	SIN11	semisup11?	?		?	.9422		?		?
	BGL14	mst2/wvec?	?		?	.9269		?		?

	Corpus questions: which converter?  which set of deprels?
	Graph parsing variants: projective vs mst?  order?  semisupervised?
	[GN13] Yoav Goldberg and Joakim Nivre. 2013. Training Deterministic Parsers with Non-Deterministic Oracles. TACL, vol 1, pp 403--414.
	[ZN11] Yue Zhang and Joakim Nivre. 2011. Transition-based Dependency Parsing with Rich Non-local Features. In ACL '11.
	[MCP05] Ryan McDonald, Koby Crammer and Fernando Pereira. 2005. Online Large-Margin Training of Dependency Parsers. In ACL '05.
	[MP06] Ryan T McDonald and Fernando CN Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In EACL '06.
	[KC10] Koo, Terry, and Michael Collins. Efficient third-order dependency parsers. ACL '10.
	[KCC08] Koo, Terry, Xavier Carreras, and Michael Collins. Simple Semi-supervised Dependency Parsing. In ACL '08.
	[SICC09] Suzuki, Jun, et al. An empirical study of semi-supervised structured conditional models for dependency parsing. In EMNLP '09.
	[SIN11] Suzuki, Jun, Hideki Isozaki, and Masaaki Nagata. Learning condensed feature representations from large unsupervised data sets for supervised learning. In ACL '11.
	[C07] Carreras, Xavier. Experiments with a Higher-Order Projective Dependency Parser. EMNLP '07.
	[E96] Jason Eisner. 1996. Three new probabilistic models for dependency parsing. In COLING-96
	[BGL14] Mohit Bansal, Kevin Gimpel and Karen Livescu. 2014. Tailoring Continuous Word Representations for Dependency Parsing. ACL '14.

2015-04-01    <dyuret@ku.edu.tr>


2015-03-31    <dyuret@ku.edu.tr>

	* Renames:
	Fmats -> Flist
	Features -> Fvec
	ArcEager -> ArcEagerR1
	ArcHybrid -> ArcHybridR1
	Pval -> Position
	Dval -> DepRel
	Mval -> Move

	* iui-k40-memory: Each K40 has 11.5GB for a total of 23GB.  The
	ZN11 setup takes in MB GPU memory:
	  280*ncpu+0.1*ncpu*nbeam*nbatch
	For ncpu=20, nbeam=64, nbatch=128 we hit the upper limit.


2015-03-29  Deniz Yuret  <dyuret@ku.edu.tr>

	* memory: Right now we spend four times the memory we need during
	parsing to store the patterns for oparse.  This means we cannot
	oparse in iui-5-0 with 1.8m moves, 2565 dims = 10k per x makes 18G
	x matrix and 4 times that does not fit in the 64GB memory.  We
	were already thinking of making x and y optional for parsers, so
	we should implement that with pre-allocation.  Started working on
	oparse.

2015-03-28  Deniz Yuret  <dyuret@ku.edu.tr>

	* archybrid13.jl: This is identical to the GN13 implementation.
	$$480169 0.50531 gpconll07h dyuret       r     03/28/2015 23:56:10 ilac.q@ilac-3-1.local              1
gpconll07hybrid13.out
DATA:	1	0.8441778	0.8279033
DATA:	2	0.83230513	0.8199081
DATA:	4	0.86308396	0.84029585
DATA:	8	0.8854879	0.8484909
DATA:	16	0.9092243	0.85968417
DATA:	32	0.9319865	0.8678793
DATA:	64	0.95570713	0.86927843
DATA:	93	0.96766937	0.8826704	***
DATA:	110	0.9719262	0.8754747

	$$480170 0.50531 opconll07h dyuret       qw    03/28/2015 23:56:36                                    1
opconll07hybrid13.out
DATA:	1	0.84368515	0.82670397
DATA:	2	0.85893685	0.8372976
DATA:	4	0.8764592	0.84829104
DATA:	8	0.89386505	0.8514891
DATA:	16	0.9144238	0.856686
DATA:	32	0.93653667	0.8646812
DATA:	64	0.9596527	0.871677
DATA:	90	0.969781	0.87687385	***
DATA:	104	0.9736997	0.87227666

	* arceager13.jl: This is identical to the GN13 implementation.
	$$480168 0.50531 bp10acl11e dyuret       qw    03/28/2015 22:10:05                                    1
bp10acl11eager13.out
DATA:	5	0.91694665	0.89882094	0.894397
DATA:	10	0.93207145	0.9077448	0.9059699
DATA:	15	0.94179016	0.9104121	0.90847504
DATA:	20	0.9500162	0.9124311	0.9120034
DATA:	25	0.95625395	0.9119825	0.9115447
DATA:	30	0.9611717	0.91415113	0.91249734	***
DATA:	35	0.9651421	0.9134033	0.9133265

	$$480171 0.00000 gpconll07e dyuret       qw    03/29/2015 00:13:46                                    1
gpconll07eager13.out
DATA:	1	0.8377936	0.82230663
DATA:	2	0.8526624	0.8237058
DATA:	4	0.88646203	0.85088944
DATA:	8	0.91018265	0.86128324
DATA:	16	0.9358134	0.87207675
DATA:	32	0.9612292	0.87327605
DATA:	39	0.9680746	0.8786728	***
DATA:	53	0.9765279	0.8772736

	$$480172 0.00000 opconll07e dyuret       qw    03/29/2015 00:13:48                                    1

	+bp10acl11eager13.out
	+bp10acl11eager.out
	+bparse64.out
	+gpconll07eager13.out: out of memory
	+gpconll07eager.out
	+gpconll07hybrid13.out
	+gpconll07hybrid3.out
	+opconll07eager13.out: out of memory
	+opconll07eager.out: crashed
	+opconll07hybrid.out

	* testbparser2.jl: Time and space for ZN11

	Space: Need ncpu x pbatch x nbeam < 7680 to fit in K20 memory
	ncpu	pbatch	nbeam	mem/cpu	acl11dev
	12	30	10	305MiB	80 secs (including 15 secs cpu initialization)
	12	40	10	310MiB	76 secs
	12	50	10	325MiB	72 secs
	12	10	64	335MiB	267 secs
	12	64	10	335MiB	72 secs
	12	80	10	350MiB	69 secs
	12	100	10	365MiB	69 secs
	12	128	10	390MiB	out of memory (this is when proc1 has 277MiB, it has more like 700 during training)

	Timing oparse: 0.07ms/w = 66s/epoch
	Timing KUnet.train: 2565x20000x24 net, 950028 words, 290s/epoch = 5mins
	!!! actually this is 448 seconds with dropout and adagrad ~ 8mins
	Timing nbeam=10: 1.5ms/w = 1425s/epoch = 24mins
	Timing nbeam=64, 6.3ms/w = 5985s/epoch = 100mins

	Recommended: ncpu=12, pbatch=32 (to be on the safe side), nbeam=10, pepochs=5.
	This will give us 5 epochs in 50 mins or 10 mins/epoch.
	x Maybe we should try learningRate=1 to speed things up?
	+ They are using nbeam=64 we are using nbeam=10
	+ acleager is not identical to acleager13

	$$480162 0.50531 bp10acl11e dyuret       r     03/28/2015 19:31:40 ilac.q@ilac-2-0.local              1
bp10acl11eager.out
DATA:	5	0.9163362	0.8975497	0.8949086	with punct
	5	0.9237522	0.9075777	0.9035607	*** (.9314 .9290 in ZN11 nopunct nbeam=64)
DATA:	10	0.9329146	0.90821844	0.90727544
DATA:	15	0.94265956	0.9111599	0.90988636
DATA:	20	0.95063406	0.9124062	0.9129384
DATA:	25	0.95681393	0.91447514	0.9135206
DATA:	30	0.96161795	0.91464967	0.9138205
DATA:	35	0.96584314	0.91492385	0.9137323
DATA:	40	0.9688472	0.9163198	0.91383815
DATA:	45	0.97186923	0.9169928	0.9142968	***
DATA:	50	0.97418284	0.915198	0.9141733

	* GN13: comparison using labeled parsers (cont. 2015-03-25)
	+ using exact feature sets acl11eager and tacl13hybrid as far as I know
	+ still not using exact archybrid13 and arceager13
	= still not using a dev set to determine when to quit
	= still not taking averages
	x still no easyfirst
	x try much larger learningRate (lr=1 worked well during debugging with conll07.tst)

	* 03281848-gpconll07eager.out:
	$$480149 0.50531 gpconll07e dyuret       r     03/28/2015 12:02:55 ilac.q@ilac-2-0.local              1
DATA:	1	0.8374577	0.82490504
DATA:	2	0.84297526	0.8139117
DATA:	4	0.8828344	0.8454927
DATA:	8	0.9080867	0.8538877
DATA:	16	0.93488634	0.8694783
DATA:	32	0.96121573	0.8762742	*** (.8869 in GN13)
DATA:	64	0.98047125	0.8686788
DATA:	65	0.98105574	0.8712772

	* 03290106-gpconll07eager.out: Second run:
	$$480164 0.50531 gpconll07e dyuret       r     03/28/2015 20:31:40 ilac.q@ilac-3-0.local              1
DATA:	1	0.8370031	0.8173096
DATA:	2	0.8489071	0.81690985
DATA:	4	0.8842317	0.85048974
DATA:	8	0.90986246	0.8630822
DATA:	16	0.93509233	0.86727965
DATA:	22	0.9471867	0.8798721	*** (.8869 in GN13)
DATA:	32	0.95996624	0.87807316
DATA:	45	0.971288	0.87227666

	* 03281640-opconll07eager.out:
	$$480150 0.50531 opconll07e dyuret       r     03/28/2015 12:11:55 ilac.q@ilac-2-1.local              1
DATA:	1	0.8356036	0.82290626
DATA:	2	0.8547718	0.8237058
DATA:	4	0.87467223	0.8374975
DATA:	8	0.8972889	0.8518889
DATA:	16	0.9231615	0.8620828
DATA:	21	0.9355335	0.8684789	*** (.8610 in GN13)
DATA:	32	0.9519026	0.85748553
DATA:	43	0.96290195	0.85968417

	Second run: opconll07eager.out
	##480165 0.50531 opconll07e dyuret       r     03/28/2015 20:33:10 ilac.q@ilac-3-1.local              1
	$$480173 0.00000 opconll07e dyuret       qw    03/29/2015 00:15:41                                    1        	

	* 03282213-opconll07hybrid.out:
	$$480151 0.50531 opconll07h dyuret       r     03/28/2015 12:12:25 ilac.q@ilac-0-0.local              1
DATA:	1	0.843506	0.8297022
DATA:	2	0.85940933	0.8392964
DATA:	4	0.87632257	0.8462922
DATA:	8	0.89458835	0.8496902
DATA:	16	0.9145694	0.85908455
DATA:	32	0.93615603	0.86707973
DATA:	64	0.96023047	0.86987805
DATA:	79	0.9661533	0.878273	*** (.8643 in GN13)
DATA:	128	0.9788523	0.8744753
DATA:	159	0.9828203	0.8746752

	$$480160 0.50531 gpconll07h dyuret       r     03/28/2015 18:58:10 ilac.q@ilac-2-1.local              1
gpconll07hybrid3.out
DATA:	1	0.84406805	0.8257046	*** (.8762 in GN13)
DATA:	2	0.83588797	0.81571054
DATA:	4	0.8616822	0.838297
DATA:	8	0.88582385	0.8502898
DATA:	16	0.9098938	0.863282
DATA:	32	0.93317556	0.8674795
DATA:	64	0.9561214	0.875075
DATA:	128	0.9758696	0.8714771
DATA:	190	0.98406756	0.8794723	***
DATA:	194	0.98414147	0.8794723


2015-03-27  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/arceager.jl: Finished and tested arceager.  Passes the
	movecosts test.  However gives different results on 9 sentences of
	conll07.tst from archybrid.  Confirmed one to be non-projective.
	They may prefer different projectivizations.  However on sentence
	35 the errors are different, hybrid=3, eager=5.

	It turns out the default move ordering matters when oracle parsing
	corpora with nonprojective sentences.  Fixed the orders that gave
	the best results (LRS for hybrid, DLRS for eager).

	+ DEBUG. *julia*
	+ fix gparser.
	+ fix bparser.
	+ train.jl does not work with arceager. *ilac-julia*

	* src/parser.jl: BEST WAY TO REPRESENT PARSERS WITH COMMON FIELDS
	BUT DIFFERENT SETS OF MOVES

	I've been wrecking my brain to figure out the best way to do this
	in Julia.  We have a set of objects (parsers in my case), with
	common state elements and constructor (buffer, stack, a set of
	arcs etc.), but different move sets (arceager, archybrid etc. see
	this if interested).  Thus functions like validmoves(), move!()
	etc. are of several different types.  So far I have considered:

	1. Just define multiple composite types with different move()
	methods (pro), listing the same fields and constructor for
	each (con).

	2. Define a single "state" type that has the necessary fields and
	constructor (pro), include a "state" as a field in multiple
	composite types requiring double reference (con: i.e. you need to
	type parser.state.field instead of just parser.field).

	3. Put the list of (common) fields in a file and include this file
	in the definition of each composite type.  This prevents the
	problem of repeating the same code, but makes me a bit uneasy.  It
	also does not solve the constructor problem (I guess I can put the
	body of the constructors in a common file and include it, but that
	makes me even more uneasy).

	4. Have a single composite type with a field that indicates the
	"move policy".  Then have functions like validmoves(), move!()
	etc. branch on this move policy.  But that's just throwing away
	the beautiful function overloading machinery of Julia.

	5. Any solutions using parametric types, singleton types etc.?

	Best response from Simon Danisch:
	I'd vote for toivo's suggestion.
	So this could be a design:

	immutable Parser{Policy}
	common
	fields
	end

	#construction
	parser = Parser{:policy1}(...)
	#specialize the functions to a policy:
	validmove(::Parser{:policy1}) = ...
	validmove(::Parser{:policy2}) = ...
	#define functions for all parsers
	move!(::Parser) = ...

	To see what the interface is:

	gparser fields: nword (not necessary), nmove (could be a fn)
	gparser methods: features, anyvalidmoves, move!, movecosts, flen
	oparser fields: head and deprel (for debugging) nword, nmove (for init)
	oparser methods: same as gparser
	the list of p is returned, so its contents need to be accessible
	+ could have head and deprel as functions as well
	+ in fact probably best never to use fields
	all parsers need to take the "parser type" as an argument
	x make sure arceasy follows this pattern
	x implement nolabel versions as parser types as well
	bparser methods: same as gparser
	bparser fields: nmove
	arc!: head, deprel, lcnt, ldep, rcnt, rdep
	move!: nmove, sptr, wptr, stack
	validmoves: nmove, nword, wptr, sptr, L1, R1
	anyvalidmoves: wptr, sptr, nword
	movecosts: nword, nmove, wptr, sptr, stack, L1, R1

2015-03-26  Deniz Yuret  <dyuret@ku.edu.tr>

	* ZN11:
	> head stack features
	Yes they are not in the paper because they did not work well. Basically I try to extract features from the root of each sub tree.
	> encoding
	I treated lengths by capping them to 8. If I remember correctly, I did not do any  binning. They can be found at depparser_macros.h
	> set of labels
	Yes you can treat them as a vector. my implementation packed everything into a integer.

	+ Actuall #2 is wrong, this is from depparser_macros.h:
	inline int encodeLinkDistance(const int &head_index, const int &dep_index) {
	static int diff;
	diff = head_index - dep_index;
	assert(diff != 0);
	if (diff<0)
	diff=-diff;
	if (diff>10) diff = 6;
	else if (diff>5) diff = 5;
	return diff;
	}

	+ He also has:
	// The size of a sentence and the words
	const unsigned MAX_SENTENCE_SIZE = 256 ;
	const unsigned MAX_SENTENCE_SIZE_BITS = 8 ;

	* TODO:
	+ Add 256 check to the code.
	+ Implement distance.
	+ Find max number of deps.
	+ Implement heads.
	x Seriously test new feature code.
	+ Rerun some old experiments with larger epoch or lower dropout.
	+ Implement arceager.
	= Run acl11 experiments.
	= Run conll07 experiments (arceager).
	+ Make latestop default for beamparser.

	+ 03271009-gptacl13.out: Testing new features and running variance experiment:
	$$480131 0.50531 gptacl13   dyuret       r     03/26/2015 19:34:40 ilac.q@ilac-2-0.local
	233	0.985691	0.8772736 (compare to .8789)

	+ 03271008-optacl13.out:
	$$480132 0.50531 optacl13   dyuret       r     03/26/2015 19:34:55 ilac.q@ilac-2-1.local
	234	0.9848177	0.86887866 (compare to .8675)

2015-03-25  Deniz Yuret  <dyuret@ku.edu.tr>

	* Summary: CONLL07 ARCHYBRID EXPERIMENTS: comparing best test
	result (should be test at best validation and should take
	averages) to GN13 archybrid result.  score1 is the old result with
	our fv022b feature set.  score2 is the new result with the
	tacl13hybrid feature set from the paper where words are replaced
	with word vectors and postags are replaced with context vectors.
	Note that this time the training is labeled but the tacl13hybrid
	features do not use the labels.

	parser	score1	score2	GN13
	oparser	.8699	.8675	.8643	03242000-optacl13.out (died at 78, best at 66)
	gparser	.8793	.8789	.8762	03250401-gptacl13.out (died at 213, best at 169)
	beam10	.8865	x	x

	Things tried:
	+ .8789 tacl13hybrid features
	+ .8789 both word+pos features
	+ .8727 unlabeled training
	+ .8709	just word features
	+ .8659 no dropout
	+ .7877 just postag features

	It seems like context vectors are a sufficient substitute for
	postags, they are also necessary, labeled training helps, and
	dropout helps a lot.

	* Experiments: conll07 archybrid experiments.  Most have not
	converged.  We need longer experiments.

	+ 03260132-gptacl13w.out: use only word vecs:
	$$480092 0.50531 gptacl13w  dyuret       r     03/25/2015 13:35:10 ilac.q@ilac-2-1.local
	256	0.9738699	0.87087744

	+ 03270146-gptacl13p.out: use only context vecs:
	$$480093 0.50531 gptacl13p  dyuret       r     03/25/2015 13:35:10 ilac.q@ilac-2-0.local
	gptacl13p.out: crashed twice!?
	187	0.8904479	0.78772736

	+ 03270848-gptacl13wp.out: use both everywhere:
	$$480094 0.50531 gptacl13wp dyuret       r     03/25/2015 13:35:40 ilac.q@ilac-3-0.local
	241	0.98982024	0.8788727

	+ 03261450-gptacl13nd.out: try no dropout:
	$$480095 0.00000 gptacl13nd dyuret       qw    03/25/2015 13:38:48
	26	0.9874197	0.8658805

	+ 03260538-gptacl13nl.out: try unlabeled training:
	$$480097 0.50531 gptacl13nl dyuret       qw    03/25/2015 13:46:17
	228	0.98530364	0.8726764

	+ try multiple runs to see variance

	+ extend training to 2x best
	+ use dev set or fixed epochs


2015-03-24  Deniz Yuret  <dyuret@ku.edu.tr>

	* GN13: experiments with matching features: gptacl13 and optacl13.
	+corpus: conll07 (deprel=19+ROOT)
	+parser: archybrid (note that archybrid13 is slightly different)
	?feats: tacl13hybrid (replaced form with word vector, postag with context vector)
	=this is a pretty crappy feature set, should also try adding word vectors for all tokens
	+training: 256 epochs static/dynamic oracle training.
	+embedding: wikipedia2MUNK-100
	+eval: including punctuation

	* gptacl13: 3m32s/epoch
	$$480081 0.50531 gptacl13   dyuret       r     03/24/2015 15:21:25 ilac.q@ilac-0-3.local
	* optacl13: 3m33s/epoch
	$$480080 0.50531 optacl13   dyuret       r     03/24/2015 15:20:10 ilac.q@ilac-2-1.local

	* TODO:
	+ add acl11eager to flist.
	+ add acl11 adapted to hybrid?
	+ use archybrid with acl11 features with labels
	+ Design better featureset for archybrid.
	+ update features to have labels, labelsets, new encoding for valence and distance
	+ s0hp, s0hw, (we don't have heads)
	+ d (s0n0, 10+ encoded, we have 4+ encoding with (-1 0 3), 8bit encodings with +-7)
	+ s0vl, s0vr (child count flat encoding, we have 3+ encoding)
	+ s0L (deprel)
	+ s0h2{w,p} grandparent
	+ s0sr, s0sl right/left label set
	+ compare acl11 with goldberg implementation: encoding etc.
	  depparser.cpp, depparser_weight.h, state.h
	  under: src/common/depparser/implementations/arceager
	+ rewrite bparser with deprels.
	+ implement arceager, make the code parser generic.
	+ create better language for specifying flists.
	+ Make xy in pxy optional in parsers.

2015-03-22  Deniz Yuret  <dyuret@ku.edu.tr>

	* DEPRELS: Unfortunately they change a lot.
	acl11: NMOD, VMOD, P, PMOD, SUB, ROOT, OBJ, AMOD, VC, SBAR, PRD, DEP (from: acl11/src/english/dependency/label/penn.h)
	conll07: NMOD, P, PMOD, SBJ, ADV, OBJ, COORD, VMOD, ROOT, AMOD, VC, IOBJ, CC, PRT, PRN, LGS, DEP, GAP, EXP, TMP
	conllWSJToken_wikipedia2MUNK-100: NMOD, P, PMOD, SBJ, OBJ, ROOT, ADV, VC, COORD, NAME, DEP, TMP, CONJ, LOC, AMOD, APPO, PRD, IM, SUB, OPRD, SUFFIX, TITLE, DIR, MNR, POSTHON, PRP, PRT, LGS, EXT, PRN, LOC-PRD, EXTR, DTV, PUT, GAP-SBJ, GAP-OBJ, DEP-GAP, GAP-TMP, GAP-PRD, PRD-TMP, PRD-PRP, BNF, GAP-LGS, GAP-LOC, DIR-GAP, LOC-OPRD, VOC, GAP-PMOD, ADV-GAP, GAP-VC, EXT-GAP, GAP-NMOD, AMOD-GAP, DTV-GAP, GAP-LOC-PRD, GAP-MNR, DIR-PRD, GAP-PRP, EXTR-GAP, MNR-PRD, LOC-TMP, GAP-OPRD, LOC-MNR, GAP-SUB, GAP-PUT, MNR-TMP, DIR-OPRD

	So we have conll2dict.jl which takes a conll corpus and construct
	dictionaries for forms, postags, and deprels.  Then we have
	conll2jld.jl which takes this dictionary and saves the conll
	corpus in a jld file replacing strings with ints (based on their
	position in dict).  Note: deprel ROOT is special, it does not
	create a new left/right move and is represented by the special
	value 0.

	* Summary:

	+ conll07 experiments: comparing best test result (should be test
	at best validation and should take averages) to GN13 archybrid
	result (we are not using deprels).

	parser	score	GN13
	oparser	.8699	.8643	03210931-opconll07.out
	gparser	.8793	.8762	03210908-gpconll07.out
	beam10	.8865	___	03221208-bpconll07.out

	+ acl11 experiments: ZN11 for comparison has word accuracy
	dev=.9314 test=.9290.  Word accuracy (excluding punct) is about 1%
	higher than head accuracy (including punct).  We are not using
	deprels.  We are using nbeam=10, they use 64.  We are using
	archybrid, they use arceager.  test1 is the test score at best dev
	result.  test2 is the best test score.

	parser	dev	test1	test2
	oparser	.9093	.9046	.9060	03212217-opacl11.out
	gparser	.9136	.9096	.9109	03212217-gpacl11.out
	beam10	.9158	.9122	.9145	03240436-bpacl11.out

	+ conllWSJToken_wikipedia2MUNK-100 experiments: Different gold
	head than acl11.  Latestop better than earlystop.
	x How about train on all mincost state/move pairs (if multiple):
	loses the ability to get precise move count.
	+ beam64 too slow, get pepochs to work.

	parser	dev	test1	test2
	beam10a	.9186	.9132	.9136	03201925-julia3_btrain_jl.out: using whole beam to train, quit at epoch=77
	beam64	.9212	.9151	.9165	bparse64.out -- stopped at epoch=166
	beam10l	.9211	.9169	.9171	03220927-bparselatestop.out: no earlystop, crashed at epoch=207
	beam10	.9205	.9150	.9166	03220548-bparsedbg.out: earlystop

	+ gfeatures experiments: Large feature sets more effective with
	nnets compared to perceptron.  Redo feature optimization and/or
	take a careful look at ZN11 and GN13 feature sets.

2015-03-21  Deniz Yuret  <dyuret@ku.edu.tr>

	* Summary:
	+ Everybody is using deprels even when training for unlabelled
	accuracy.
	+ ZN11 uses different heads than Husnu&Volkan's dataset.  I have
	created acl11 files to match ZN11.
	+ Nnets seem to handle extra features better than perceptrons,
	high dimensional feature sets seem to do well.
	+ We are better than GN13 even without deprels, however this is
	directly optimizing on test data, we need to use dev.
	+ We have the code for ZN11 and GN13, we can take a look at the
	feature sets in detail.
	+ Suzuki uses 3.5B words for semisupervised learning, we should
	grow our LM and SCODE corpora as well.
	+ ZN11 uses arceager, GN13 best result with arceasy.

	* ZN11: (Zhang&Nivre11) nbeam=64 dev=.9314 tst=.9290
	wordacc (excluding punct) conv=Penn2Malt.  parser=arceager.
	learner=perceptron.  uses labels to improve link accuracy!  (seems
	they get .65) also check out their feature set, and their distance
	encoding.  Their whole dataset is projective!

	* GN13: (Goldberg&Nivre13) .8643/.8762 static/dynamic archybrid on
	conll07 headacc (including punct).  no beam.  I think uses labels,
	but better check feature specs.

	* TODO: parser and training
	+ ** add deprel labels as features! **: features.jl, archybrid.jl
	= output two variables (move+label) or one (movelabel)?
	+ check features and encodings of zhang&nivre11.
	+ use oparser for fselect.  faster if only test at the end.
	+ need fselect again for labeled parser.
	+ Try arceager (Zhang&Nivre11).
	+ train.jl: write pepochs option to get nbeam=64 to work.
	x train.jl: implement earlystop for efficiency (Zhang does it)
	x latestop seems to perform better?
	+ train.jl: longer training (stop if 2x best epoch)
	+ train.jl: save models
	+ train.jl: compute eval.py (wordacc) score.
	= can we use rnn's to learn parser-action sequences? prepare
	  ordered data for ozan.
	= better vectors: fastsubs and scode on bigger better data.  which
	  data did volkan use? onur interested. sent email.
	+ are they (Goldberg,Zhang) using the deprels during parsing?
	  (BILOU is better than BIO) YES!
	+ which arc system are they (Zhang&Nivre11) using? arceager.

	* TODO: model and optimization
	x play with optimization, rmsprop, adam, layers, dropout, hidden layers?
	x alternative to adagrad?
	+ try more hidden layers.

	* TODO: features
	= fv1768: memory error
	x try bparse 10 epochs all Flist: oparse instead for fselect.
	+ try local search around best feats
	+ feature select with acl11
	+ feature select with conll07
	+ features from ZN11 and GN13

2015-03-20  Deniz Yuret  <dyuret@ku.edu.tr>

	* ~/src/acl11: Got Yue Zhang's acl11 data.  Heads are different!
	+ ** generate new dataset with acl11 heads. **
	+ generate new dataset with conll07.

	Note: fv021a may not be ideal for acl11! run gfeatures after fixing bugs.
	Note: using fv022b for conll07.

	* 03210931-opconll07.out:
	$$480064 0.50531 opconll07  dyuret       r     03/20/2015 18:39:10 ilac.q@ilac-0-0.local
	1	0.8383803	0.8205077
	2	0.8559474	0.841695
	4	0.87357944	0.8420947
	8	0.8902576	0.84929043
	16	0.91164714	0.8560864
	32	0.9369286	0.85808516
	64	0.9609493	0.86128324
	128	0.97914565	0.86128324
	170	0.98390406	0.86987805	*** (compare to GN13=.8643)
	256	0.9885506	0.86508095

	* 03210908-gpconll07.out:
	$$480065 0.50531 gpconll07  dyuret       r     03/20/2015 18:45:10 ilac.q@ilac-3-0.local
	1	0.8376861	0.8237058
	2	0.79815173	0.77953225
	4	0.87328166	0.85108936
	8	0.89544153	0.8604837
	16	0.91665864	0.86667997
	32	0.9414877	0.8712772
	64	0.964232	0.8702778
	128	0.9813267	0.8786728
	133	0.98199844	0.87927246	*** (compare to GN13=.8762)
	256	0.98990756	0.8724765

	* 03221208-bpconll07.out:
	$$480066 0.50531 bpconll07  dyuret       r     03/20/2015 18:45:55 ilac.q@ilac-2-1.local
	$ grep DATA bpconll07.out | awk '$2>=a{print;a=$2*2}'
	1	0.8489049	0.836698
	2	0.86899793	0.8554867
	4	0.8904457	0.8686788
	8	0.9080262	0.875075
	16	0.92726606	0.88227063
	32	0.94914156	0.88127124
	54	0.9653002	0.8864681	***
	64	0.9700833	0.8772736
	128	0.9850327	0.875075
	256	0.9918983	0.8680791

	* 03200918-gfeatures_sh.out: experiment with features (honnibal
	comment).  This is on husnu deps.
	+ try gparse 10 epochs all Flist
	$$479993 0.50531 _gfeatures dyuret       r     03/19/2015 10:46:40 ilac.q@ilac-2-3.local
	$$479998 0.50531 _gfeatures dyuret       r     03/19/2015 11:46:40 ilac.q@ilac-1-3.local
	= try to debug and fix the crashes
	$$480067 0.50531 gfeatures2 dyuret       qw    03/20/2015 19:19:08
	$$480068 0.50531 gfeatures3 dyuret       qw    03/20/2015 21:53:02
	$$480069 0.50531 gfeatures3 dyuret       r     03/22/2015 13:12:40 ilac.q@ilac-2-2.local
	$ egrep '^(DATA:.10|julia3)' _gfeatures_sh.out
	fv1768	0.92688215	0.9029838	0.8982076	(3348 dims, 865MiB)
	fv031a	0.9175582	0.90168756	0.8972373	(2512 dims)
	fv034	0.92510116	0.9013386	0.8971491	(2452 dims, 660MiB)
	fv021a	0.9179561	0.90113914	0.89709616	(1326 dims)
	fv084	0.92291176	0.9010644	0.8951027	(3138 dims)
	fv022a	0.918115	0.9009896	0.8971138	(1426 dims)
	fv023	0.9185719	0.9005908	0.8966375	(1526 dims)
	fv022	0.9171161	0.9003415	0.8965669	(1516 dims)
	fv102	0.9187382	0.90001744	0.8975549	(4638 dims)
	fv022b	0.916714	0.8999177	0.89713144	(1424 dims)
	fv023a	0.91787714	0.8998679	0.89737844	(1616 dims)
	fv018	0.91692454	0.8995937	0.8934091	(1608 dims)
	fv804	0.91603404	0.89899546	0.89492625	(1604 dims)
	fv021	0.91659826	0.89894557	0.8955437	(1416 dims)
	fv708	0.9135615	0.89847195	0.89399123	(1408 dims)
	fv019	0.915633	0.8984221	0.89533204	(1411 dims)
	fv015a	0.914752	0.8978488	0.8922976	(1401 dims)
	fv018a	0.9154783	0.89777404	0.89423823	(1407 dims)
	fv017	0.9170309	0.8975247	0.89513797	(1508 dims)
	fv039	0.9245001	0.897450	0.89577305	(2744 dims, 727MiB)
	fv130	0.91527617	0.8966772	0.89319736	(3538 dims, 909MiB)
	fv808	0.91797924	0.89655256	0.89259756	(1608 dims)
	fv017a	0.916613	0.89615375	0.893021	(1508 dims)
	fv136	0.9144846	0.8956053	0.8926505	(3544 dims, 910MiB)
	fv012	0.9068354	0.88982224	0.88806367	(1200 dims)
	fv008	0.9009608	0.8860832	0.88349444	(800 dims)
	fv008w	0.8957652	0.8806242	0.8796309	(800 dims)


2015-03-19  Deniz Yuret  <dyuret@ku.edu.tr>

	* 03201925-julia3_btrain_jl.out: Train using beam parser. ~1h per
	epoch.  Training on the whole beam with nbeam=10.
	+ merge mincostpath with master after this one finishes.
	$$479844 0.50531 julia3_btr dyuret       r     03/17/2015 12:01:55 ilac.q@ilac-0-3.local
	1	0.8714639358032035	0.8880773736819803	0.8838649354315151
	2	0.8950433643212408	0.8938604581598824	0.890762825488674
	4	0.9122091687400475	0.903033626642072	0.8975901488956319
	8	0.9273346259121167	0.9078694817658349	0.9040117140639334
	16	0.9424559294340376	0.912007378418127	0.9076811798743913
	32	0.9569711670068192	0.9151481915397462	0.9107331874955896
	64	0.9691327587380277	0.9175910461898945	0.9127443370263214
	74	0.9711445121199407	0.9180397337786973	0.9135734951661845	*t*
	77	0.9718000903088589	0.9186130568088342	0.9132030202526286	*d*

	* 03290018-bparse64.out: experiment with larger beams 1h22m/epoch
	$$479988 0.50531 bparse64   dyuret       r     03/19/2015 09:48:25 ilac.q@ilac-1-0.local
	1	0.88225925	0.8752399	0.87107474
	2	0.8944715	0.8857093	0.8815892
	4	0.9085364	0.8967271	0.8914861
	8	0.9206171	0.90582544	0.9008715
	16	0.93292725	0.9116833	0.9058465
	32	0.9474447	0.9167186	0.91170347
	64	0.96225166	0.91851336	0.9141204
	113	0.97382075	0.92115563	0.9150907	*d*
	128	0.9761996	0.9197597	0.9152671
	156	0.9793417	0.91841364	0.9164667	*t*
	166	0.98041004	0.9195603	0.91507304	stopped

	* 03220927-bparselatestop.out: why is this different from static
	oracle training?  we are rewarding mincost state/move pairs on the
	beam.  are the mincost states on the beam that different than the
	mincost states for the static oracle?  why not use earlystop in
	gparser?  **why not follow the whole mincostpath and not use
	earlystop in bparser?**

	$$480009 0.50531 bparselate dyuret       r     03/19/2015 14:17:25 ilac.q@ilac-2-2.local
	$$480024 0.50531 bparselate dyuret       r     03/19/2015 17:07:40 ilac.q@ilac-2-2.local
	1	0.8824845	0.8760874	0.8711453
	2	0.8931568	0.8864322	0.8810599
	4	0.9077248	0.8963033	0.89115095
	8	0.92096025	0.9067976	0.8996013
	16	0.9335746	0.9120572	0.90686965
	32	0.94735837	0.9163447	0.91173875
	64	0.9624811	0.9194606	0.9139969
	121	0.9752039	0.9198843	0.9171371	*t*
	128	0.9760491	0.92015857	0.91664314
	129	0.97622174	0.9211307	0.9169078	*d*
	207	0.9833121	0.9198843	0.91528475	crashed

	* 03220548-bparsedbg.out: mincostpath bparser results: at most one
	xy pair per beam (the first one which has the initial mincost) is
	added to the training set. nbeam=10.  (18m50s per epoch) Note: trn
	improvement is slower compared to whole beam training.  dev/tst
	catches up and passes after 32 epochs.  Less overfitting?  Speed
	3x faster.

	$$479933 0.50531 bparsedbg  dyuret       r     03/18/2015 21:32:10 ilac.q@ilac-0-1.local
	1	0.8811414	0.8743675	0.87079245
	2	0.8940389	0.8873545	0.8804777
	4	0.9079269	0.89695144	0.8927387
	8	0.92069286	0.9061495	0.89916027
	16	0.9332304	0.9117581	0.9058112
	32	0.94746995	0.91639453	0.91230327
	64	0.9625506	0.9181644	0.9151789
	128	0.97627544	0.91808957	0.91576105
	130	0.97652704	0.9197597	0.9165726	*t*
	193	0.982411	0.92053246	0.91503775	*d*
	256	0.985992	0.9191365	0.91507304

	* 03240436-bpacl11.out: Trying acl11 dependencies.
	$$480062 0.50531 bpacl11    dyuret       r     03/20/2015 18:27:55 ilac.q@ilac-2-3.local
	1	0.87517524	0.86920756	0.86677015
	2	0.88630337	0.8795274	0.87807846
	4	0.9019197	0.891617	0.8888046
	8	0.9159751	0.8992198	0.8985252
	16	0.92985153	0.9061495	0.9052113
	32	0.94513106	0.91220677	0.9099746
	64	0.9617148	0.9144004	0.9116329
	79	0.9671715	0.9158212	0.9121622	*d* (ZN11:9314w)
	121	0.9770028	0.91380215	0.9145438	*t* (ZN11:9290w)
	128	0.97827536	0.91502357	0.91406745
	256	0.99009085	0.9123564	0.9130795

	* TODO:
	+ headpct-wordpct diff around 1% according to vectorparser/ChangeLog.
	= compare Penn2Malt conversion with Zhang: emailed Zhang.
	+ check Suzuki11.
	+ merge mincostpath and structure code to experiment with other variants.
	= parse conll07 for static/dynamic comparison.
	= experiment with subsets of the beam to use for training (zhang email)?

	* Chen&Zhang&Zhang14: .9422 is best from related work Suzuki11 in
	http://www.aclweb.org/anthology/C14-1078. Conversion with
	Penn2Malt.  Uses Carreras07 decoder with MIRA and Bohnet2010 features.

	* Suzuki11: (condensed feature representations, semisupervised)
	cites dev/tst Suzuki11:9433/9422, Koo08:9330/9316, Chen09:?/9316,
	Suzuki09:9413/9379.  Uses 3.7b token for unsupervised learning (as
	Suzuki09).  Suzuki 09 uses conv=yamada parser=2nd-order.

	* Koo08: tst23=.9316 (excluding punct).  Uses Carreras07 (second
	order).  Cluster info adds +1.14.


2015-03-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* todo:
	= write bparser:mincostpath - testing on *julia*
	= add options to gtrain/btrain - testing on {ogb}parsedbg.out
	+ try onur's problem
	x try bparser without dropout: no reason to do it with mincostpath, input similar to oparser.
	= try gparser with other fvectors

2015-03-16  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	+ solve memory problems: x=10GB, bparse=100GB!
	+ save networks: implement savenet, testnet or copy:save copy:test
	= beam earlystop? only train with mincostpath?  same with greedy?
	= play with features

	* memory:
	x do we need to wait for @everywhere gc()?
	x whos() does not work from inside meminfo: turn into macro?: still does not show locals
	= check out getBytes from http://stackoverflow.com/questions/28402989/check-size-in-bytes-of-variable-using-julia
	x if restarting workers does not help, just write code without DArrays.
	x maybe forget about workers and write multithreaded.
	+ but DArrays are only used for corpora, not the returned x!?  help debug gc().

2015-03-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* upper-bound: These are head accuracies with oracle parser (less than 100% because of nonplanar trees?)
	evalheads(p1,trn) => 0.9940970160879469
	evalheads(p2,dev) => 0.9950395094349029
	evalheads(p3,tst) => 0.9935960764942489

	* 03160543-julia3_otrain_jl.out: Train using static oracle. 4m07s
	per epoch. (5m50s with testing)

	epoch	trn			dev			tst
	1	0.8739110847259238	0.8660916818306453	0.8615129489803119
	2	0.8855433734584665	0.8766607672557769	0.8707218968315574
	4	0.8988882432938818	0.8874541964753098	0.8828064356784984
	8	0.9100005473522885	0.8930877184236109	0.8891574341965987
	16	0.9229485867784949	0.9015130742577959	0.8943264413238303
	32	0.9391302151094494	0.9064486377346263	0.9001481899654223
	64	0.9567865368178622	0.9097639404741132	0.903288405899372
	128	0.9730323737826675	0.9098636488271805	0.9051054971420507
	256	0.9837520578340849	0.9088665652965077	0.9075047632488886

	* 03200328-oparsedbg.out: Replicating otrain results with new train.jl (6m22s/epoch)
	$$479920 0.50500 oparsedbg  dyuret       r     03/18/2015 19:55:10 ilac.q@ilac-0-2.local
	1	0.8737332	0.86576766	0.8628537
	2	0.8857002	0.87501556	0.8715687
	4	0.89788723	0.88446295	0.8803366
	8	0.91024685	0.8940349	0.88966906
	16	0.9238307	0.901538	0.89517325
	32	0.93902916	0.9065733	0.8999365
	64	0.95688546	0.90904105	0.9024416
	128	0.97318393	0.91016275	0.9060052
	254	0.98392576	0.9116335	0.9063228	*d*
	255	0.9839089	0.91013783	0.90736365	*t*
	256	0.98388153	0.910786	0.90690494

	* 03212217-opacl11.out: Trying the ACL11 Zhang dependencies.
	$$480059 0.50531 opacl11    dyuret       r     03/20/2015 18:15:10 ilac.q@ilac-2-0.local
	1	0.86862916	0.8634993	0.86227155
	2	0.88202244	0.87424284	0.87324464
	4	0.8953294	0.8846873	0.8833357
	8	0.90825427	0.8924396	0.8921036
	16	0.92300224	0.89936936	0.8979077
	32	0.9404091	0.9041554	0.9030238
	64	0.95960015	0.90749556	0.9038882
	128	0.9779785	0.908044	0.90415287
	205	0.9867804	0.9072712	0.90602285	*t*
	210	0.9871225	0.9092903	0.9046115	*d*
	256	0.9896782	0.9065234	0.9043998

	* 03171146-julia3_gtrain_jl.out: Train using greedy parser. 6m30s per epoch.
	1	0.8740110817786423	0.8695565470997333	0.8623244654576248
	2	0.8796424947475232	0.8704539222773388	0.8683755557123704
	4	0.8938568126413117	0.8808235909963357	0.8785018700162304
	8	0.9140688484970969	0.8979734277239075	0.8951732411262437
	16	0.9277031834851184	0.9051773562330184	0.9003246065909252
	32	0.941745927488453	0.9094149612383777	0.9050349304918496
	64	0.9574433595641392	0.9112595657701223	0.9091101545409639
	93	0.9657357467358857	0.9154223895106812	0.9110330957589443
	127	0.9721671361265142	0.9149986290101453	0.9128678286641733

	* 03192351-gparsedbg.out: Replicating gtrain results (6m32s/epoch) with new train.jl
	$$479926 0.50531 gparsedbg  dyuret       r     03/18/2015 19:58:55 ilac.q@ilac-0-0.local
	1	0.87359005	0.86753744	0.8616188
	2	0.88026565	0.87152576	0.86717594
	4	0.89791775	0.8860832	0.88430595
	8	0.9142257	0.90056586	0.8936737
	16	0.92736214	0.90445447	0.8997248
	32	0.9417891	0.90948975	0.9047703
	64	0.95761913	0.91292965	0.9092689
	128	0.9720545	0.91372734	0.91122717
	194	0.9796385	0.9158212	0.9123562	*d*
	238	0.98255104	0.9152479	0.9139616	*t*
	256	0.98354787	0.9134033	0.91325593

	* 03212217-gpacl11.out: Trying the acl11 dependencies.
	$$480058 0.50531 gpacl11    dyuret       r     03/20/2015 18:14:25 ilac.q@ilac-1-3.local
	1	0.8675871	0.8654436	0.86175996
	2	0.85359275	0.8475958	0.8446828
	4	0.8899264	0.87940276	0.87911934
	8	0.91075844	0.89585465	0.8929857
	16	0.92462856	0.901887	0.89970714
	32	0.94124806	0.90712166	0.90517604
	64	0.9581886	0.9111848	0.90812224
	116	0.9726871	0.9120572	0.9109096	*t*
	128	0.9751281	0.9124561	0.90872204
	184	0.98290575	0.9135778	0.90960413	*d*
	256	0.98849404	0.9113094	0.9093748

2015-03-14  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/oparser.jl: Oracle parser: useful for generating static oracle training data.
	julia> @time o2=KUparser.oparse(dev,feats,20);
	elapsed time: 2.306022569 seconds (430 MB allocated, 44.86% gc time in 6 pauses with 3 full sweep)
	julia> mean(vcat(o1[1]...) .== vcat(map(x->x.head,dev)...))
	0.9950395094349029

	* src/gparser.jl:
	x do not need to allocate p, y, etc. for the whole corpus in batch version.

	* TODO:
	+ return xy from gparser and bparser.
	+ train. (need logploss, need earlystop?, try rmsprop?, adam?: adagrad may not do well on changing data)
	+ static oracle
	+ greedy parser
	+ beam parser: still has memory issues
	= feature select.

	* INFO:
	+ biyofiz gpu memory is sufficient for nbeam*nbatch*ncpu <= 10000

	julia> @time b42y=KUparser.bparse(dev,net,feats,100,5,20);
	elapsed time: 261.986853539 seconds (3 MB allocated)
	julia> @time b41y=KUparser.bparse(dev,net,feats,10,50,20);
	elapsed time: 32.315438529 seconds (3 MB allocated)
	julia> find(b41y .!= b42y)
	1-element Array{Int64,1}:
	1324

	+ beam parser gives better head score with same model:

	julia> gold=map(x->x.head, dev);
	julia> mean(vcat(gold...) .== vcat(g0...))
	0.9105865343869183
	julia> mean(vcat(gold...) .== vcat(b41y...))
	0.9154473165989481

	* TODO: src/bparser.jl: minibatch version:
        x do not keep computing costs for finished sentences: too minor
        x do all beam elements finish simultaneously?
        x cache cost in parser?
	x repeated states on the beam?
	= nbeam=10 and nbeam=100 give the same parse?
	== in all except sent 1324. beam1 vs beam10 differ in 224 sentences.

	* test/timing.jl: minibatch version speed test:

	# Greedy parser
	g0=KUparser.gparse(dev,gnet,feats)		elapsed time: 148.237107122 seconds
	g1=KUparser.gparse(dev,gnet,feats,1700)		elapsed time: 22.188889154 seconds
	g2=KUparser.gparse(dev,gnet,feats,1700,20)	elapsed time: 8.051600384 seconds
	g3=KUparser.gparse(dev,gnet,feats,100)		elapsed time: 26.318353452 seconds
	g4=KUparser.gparse(dev,gnet,feats,100,20)	elapsed time: 8.062792516 seconds

	# These are with beam size 1
	# Final parses same as g0
	b0=KUparser.bparse(dev,gnet,feats,1)		elapsed time: 156.000890758 seconds
	b1=KUparser.bparse(dev,gnet,feats,1,1700)	elapsed time: 30.145607698 seconds
	b2=KUparser.bparse(dev,gnet,feats,1,1700,20)	elapsed time: 8.18903106 seconds
	b3=KUparser.bparse(dev,gnet,feats,1,100)	elapsed time: 31.510566003 seconds
	b4=KUparser.bparse(dev,gnet,feats,1,100,20)	elapsed time: 8.234869093 seconds

	# These are with beam size 10 on the first 100 sentences of dev:
	# Final parses not same with g0[1:100]
	b01=KUparser.bparse(dev[1:100],gnet,feats,10)		elapsed time: 25.057099378 seconds
	b11=KUparser.bparse(dev[1:100],gnet,feats,10,100)	elapsed time: 14.705737159 seconds
	b21=KUparser.bparse(dev[1:100],gnet,feats,10,100,20)	elapsed time: 5.154158539 seconds
	b31=KUparser.bparse(dev[1:100],gnet,feats,10,10)	elapsed time: 16.469459534 seconds
	b41=KUparser.bparse(dev[1:100],gnet,feats,10,10,20)	elapsed time: 5.161457464 seconds

	# These are with beam size 100 on the first 100 sentences of dev:
	# Final parses same as b01
	b02=KUparser.bparse(dev[1:100],gnet,feats,100)		elapsed time: 137.201635533 seconds
	b12=KUparser.bparse(dev[1:100],gnet,feats,100,100)	elapsed time: 132.684041541 seconds
	b22=KUparser.bparse(dev[1:100],gnet,feats,100,100,20)	elapsed time: 18.501004294 seconds
	b32=KUparser.bparse(dev[1:100],gnet,feats,100,10)	elapsed time: 132.337806865 seconds
	b42=KUparser.bparse(dev[1:100],gnet,feats,100,10,20)	elapsed time: 18.514346591 seconds


2015-03-13  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/bparser.jl: speed test:

	# Time for greedy parser on 1700 sentence dev
	julia> @time g=KUparser.gparse(dev,gnet,feats);
	elapsed time: 149.497789522 seconds (4116 MB allocated, 1.06% gc time in 189 pauses with 0 full sweep)

	# Time for beam parser on same data with beam=1
	julia> @time b=KUparser.bparse(dev,gnet,feats,1);
	elapsed time: 159.778490775 seconds (7173 MB allocated, 3.76% gc time in 328 pauses with 9 full sweep)

	# Time on dev[1:100]
	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,1);
	elapsed time: 9.962360704 seconds (440 MB allocated, 5.42% gc time in 20 pauses with 1 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,5);
	elapsed time: 16.47657518 seconds (1940 MB allocated, 8.55% gc time in 89 pauses with 2 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,10);
	elapsed time: 27.863031912 seconds (3773 MB allocated, 12.66% gc time in 172 pauses with 6 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,50);
	elapsed time: 84.229235102 seconds (17997 MB allocated, 18.13% gc time in 806 pauses with 25 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,100);
	elapsed time: 158.230640377 seconds (35313 MB allocated, 16.89% gc time in 1549 pauses with 41 full sweep)

	# Time tests with ncpu=20

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,1,20);
	elapsed time: 9.237851461 seconds (2 MB allocated)
	elapsed time: 9.766501577 seconds (107 MB allocated, 4.21% gc time in 3 pauses with 1 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,5,20);
	elapsed time: 9.447347633 seconds (2 MB allocated)
	elapsed time: 9.975221305 seconds (107 MB allocated, 4.11% gc time in 3 pauses with 1 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,10,20);
	elapsed time: 13.658449233 seconds (2 MB allocated)
	elapsed time: 14.184126282 seconds (107 MB allocated, 2.89% gc time in 3 pauses with 1 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,50,20);
	elapsed time: 15.286421047 seconds (2 MB allocated)
	elapsed time: 15.809166647 seconds (107 MB allocated, 2.60% gc time in 3 pauses with 1 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,100,20);
	elapsed time: 27.447839359 seconds (2 MB allocated)
	elapsed time: 28.380576273 seconds (107 MB allocated, 2.96% gc time in 5 pauses with 2 full sweep)


2015-03-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO: src/bparser.jl:
	+ early stop?  could implement by copying into big x
	+ mincost may not be 0 for non-planar sentences
	= sortperm! suggest AbstractArray to julia
	= Now we sort the nc candidates and copy top np back to the beam
	x softperm! may not be necessary when nc <= beam (unless we care about the first candidate being the best)
	= do we need features when we have 0 or 1 valid move?
	= do not return y from gparser, return x,z consistently from each method
	x maybe have optional trn struct and return trn.x trn.y trn.nx
	x check for identical states on the beam: score will be difft

2015-03-08  Deniz Yuret  <dyuret@ku.edu.tr>

	* Speed-test:

	julia@iui> @time KUparser.gparse(trn, net, feats, 2000, 20);
	elapsed time: 40.50693341 seconds (9502 MB allocated, 8.38% gc time in 14 pauses with 11 full sweep)
	elapsed time: 86.205662824 seconds (107430 MB allocated, 6.12% gc time in 86 pauses with 14 full sweep)

	julia@bio> @time KUparser.gparse(trn, net, feats, 500, 20);
	elapsed time: 75.61331844 seconds (9476 MB allocated, 3.19% gc time in 8 pauses with 6 full sweep)
	elapsed time: 112.552530706 seconds (107352 MB allocated, 4.21% gc time in 75 pauses with 9 full sweep)

	julia3@ilac> @time KUparser.gparse(trn, net, feats, 1700, 12);
	elapsed time: 86.231203 seconds (11419169336 bytes allocated, 7.88% gc time)
	elapsed time: 123.933778 seconds (75581738656 bytes allocated, 16.60% gc time)

2015-03-05  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	= write bparser
	= real use of rnn's is to map text to meaning!

	= should we try parallel for?
	= see how SharedArray is implemented and use it for net weights: x is bigger!

	* src/gparser.jl: gpu minibatch version:

	# dev has 1700 sentences and 40117 words
	# fastest matlab test speed was 26 sents/sec = 1.6298 ms/word
	# fastest matlab dump speed was 30 sents/sec = 1.4125 ms/word
	# fastest julia test speed is 80 sents/sec = 0.5286 ms/word
	# fastest julia dump speed is 116 sents/sec = 0.3652 ms/word

	# 3.6986 ms/word: parsing sentences one at a time
	include("fooparser.jl")
	julia> @time h2=KUparser.gparse(dev, n, Feats.fv021a);
	elapsed time: 148.37933571 seconds (4145 MB allocated, 0.83% gc time in 190 pauses with 0 full sweep)

	# 0.5286 ms/word: parsing sentences in minibatches
	julia> @time h1=KUparser.gparse(dev, n, Feats.fv021a, 1700);
	elapsed time: 21.206189373 seconds (6355 MB allocated, 8.16% gc time in 273 pauses with 1 full sweep)

	# 0.3652 ms/word: everything except KUnet prediction
	julia> @time h0=KUparser.gparse(dev, n, Feats.fv021a, false);
	elapsed time: 14.648858577 seconds (3488 MB allocated, 5.64% gc time in 160 pauses with 0 full sweep)

	# For training with Julia I expect 8 mins to parse, 2 mins to
	update, 10 mins per epoch.

2015-03-04  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/features.jl:
	> @time foo=KUparser.gparse(dev[1:100], n, Feats.fv021a); # with rand!
	> elapsed time: 0.899239837 seconds (212 MB allocated, 4.95% gc time in 9 pauses with 0 full sweep)

	= Better but still allocating, why?

	> @time foo=KUparser.gparse(dev[1:100], n, Feats.fv021a); # with predict
	> elapsed time: 9.124658661 seconds (254 MB allocated, 0.79% gc time in 11 pauses with 0 full sweep)

	* features.jl: get rid of avec allocation.
	* net.jl: find what allocates 6kb?

2015-03-03  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	= get rid of allocation
	= optimize feature extraction
	x cuda parser?
	x solve pmap problem (shared arrays, pointers?)

	* timing: dev is the 1700 sentences:
	sent_pct: 0.5888
	head_pct: 0.0894
	word_pct: 0.0785
	move_pct: 0.0340

	We are doing about 11 sentences/second: (90ms/sent)

	julia> @time foo=KUparser.gparse(dev[1:10], net, Fmats.fv021a)
	elapsed time: 1.078577283 seconds (65 MB allocated, 0.85% gc time in 3 pauses with 0 full sweep)
	julia> @time foo=KUparser.gparse(dev[1:100], net, Fmats.fv021a);
	elapsed time: 9.161665507 seconds (557 MB allocated, 0.80% gc time in 25 pauses with 0 full sweep)
	julia> @time foo=KUparser.gparse(dev, net, Fmats.fv021a);
	elapsed time: 152.361663243 seconds (9084 MB allocated, 1.26% gc time in 415 pauses with 0 full sweep)

	The feature construction is actually pretty fast: (10ms/sent)

	After replacing "predict" with "rand":
	julia> @time foo=KUparser.gparse(dev[1:100], net, Fmats.fv021a);
	elapsed time: 1.007098236 seconds (510 MB allocated, 11.25% gc time in 23 pauses with 0 full sweep)

	However there is too much allocation and speed can be improved.

	More impact would be to batch sentences first.

2014-10-27  Deniz Yuret  <dyuret@ku.edu.tr>

	* run_static_dynamic_1023_archybrid_conllWSJToken_wikipedia2MUNK_100_fv021a_0_333140404105682_ilac.out:
	First two epochs identical to 1017 run.  For conll07 use the 2014-10-10 result.
	:(copied from vectorparser/ChangeLog)

		stat_pct		move_pct	      	head_pct	      	word_pct
	epoch	trn	dev	tst	trn	dev	tst   	trn	dev	tst   	trn	dev	tst
	1	0.0142	0.0329	0.0346	0.0203	0.0409	0.0423	0.0590	0.1081	0.1134	0.0509	0.0946	0.0983
	2	0.0138	0.0376	0.0389	0.0136	0.0373	0.0374	0.0400	0.0994	0.1023	0.0359	0.0881	0.0912
	3	0.0102	0.0364	0.0380	0.0081	0.0351	0.0355	0.0269	0.0934	0.0969	0.0246	0.0832	0.0860
	4	0.0088	0.0361	0.0378	0.0058	0.0348	0.0352	0.0209	0.0921	0.0955	0.0193	0.0819	0.0845
	5	0.0076	0.0360	0.0376	0.0043	0.0342	0.0349	0.0174	0.0910	0.0950	0.0162	0.0810	0.0839
	6	0.0071	0.0359	0.0375	0.0033	0.0341	0.0347	0.0150	0.0907	0.0940	0.0142	0.0807	0.0835
	7	0.0068	0.0359	0.0374	0.0025	0.0336	0.0346	0.0131	0.0894	0.0940	0.0123	0.0795	0.0835
	8	0.0067	0.0360	0.0374	0.0023	0.0336	0.0343	0.0125	0.0892	0.0934	0.0118	0.0792	0.0832
	9	0.0068	0.0359	0.0377	0.0023	0.0333	0.0342	0.0125	0.0888	0.0935	0.0119	0.0789	0.0833
	10	0.0068	0.0358	0.0376	0.0021	0.0333	0.0342	0.0121	0.0885	0.0933	0.0116	0.0786	0.0830
	11	0.0068	0.0360	0.0377	0.0021	0.0335	0.0341	0.0120	0.0891	0.0928	0.0116	0.0792	0.0827
	12	0.0068	0.0361	0.0378	0.0021	0.0335	0.0341	0.0119	0.0889	0.0925	0.0116	0.0790	0.0824
	13	0.0068	0.0359	0.0379	0.0020	0.0332	0.0339	0.0117	0.0884	0.0920	0.0114	0.0785	0.0821
	14	0.0069	0.0359	0.0380	0.0020	0.0333	0.0340	0.0116	0.0888	0.0921	0.0112	0.0788	0.0823
	15	0.0068	0.0360	0.0380	0.0019	0.0334	0.0340	0.0115	0.0888	0.0920	0.0111	0.0787	0.0820
	16	0.0068	0.0364	0.0380	0.0019	0.0338	0.0340	0.0116	0.0899	0.0919	0.0113	0.0798	0.0820
	17	0.0069	0.0364	0.0379	0.0019	0.0337	0.0340	0.0114	0.0898	0.0918	0.0110	0.0799	0.0819
	18	0.0069	0.0363	0.0380	0.0018	0.0336	0.0341	0.0115	0.0894	0.0922	0.0110	0.0795	0.0823
	19	0.0069	0.0365	0.0380	0.0018	0.0336	0.0341	0.0115	0.0893	0.0921	0.0109	0.0795	0.0822
	20	0.0069	0.0364	0.0380	0.0017	0.0337	0.0342	0.0113	0.0894	0.0927	0.0108	0.0796	0.0827

2014-10-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* features: only 11 common, marked by '+' below.

	>> c7 = load('archybrid_conll07EnglishToken_wikipedia2MUNK-100_rbf376678_1014_cache.mat');
	>> c8 = load ('archybrid_conllWSJToken_wikipedia2MUNK-100_rbf333140_cache.mat');
	>> [c71, c72, c73] = run_sort_bestfeats(c7.cache);
	>> [c81, c82, c83] = run_sort_bestfeats(c8.cache);


	fv022b/conll07				fv021a/conllWSJ

	+[-208]    '-n0w'       '[0 0 4]'   	+[-157]    '-s0w'       '[-1 0 4]'
	+[-152]    '-s0w'       '[-1 0 4]'  	+[-141]    '-n0w'       '[0 0 4]'
	+[ -69]    '-s1w'       '[-2 0 4]'  	+[ -70]    '-s1w'       '[-2 0 4]'
	+[ -68]    '-n1w'       '[1 0 4]'   	+[ -55]    '-n1w'       '[1 0 4]'
	+[ -60]    '-n0l1w'     '[0 -1 4]'  	-[ -21]    '-s2w'       '[-3 0 4]'
	+[ -50]    '-n1c'       '[1 0 -4]'  	+[ -21]    '-s0l1w'     '[-1 -1 4]'
	+[ -40]    '-s0c'       '[-1 0 -4]' 	+[ -21]    '-s0c'       '[-1 0 -4]'
	+[ -34]    '-s0r1w'     '[-1 1 4]'  	+[ -18]    '-n0l1w'     '[0 -1 4]'
	+[ -33]    '-n0c'       '[0 0 -4]'  	+[ -18]    '-n1c'       '[1 0 -4]'
	-[ -32]    '-s0r<'      '[-1 0 6]'  	-[ -16]    '-s1r<'      '[-2 0 6]'
	-[ -31]    '-s1d>'      '[-2 0 7]'  	-[ -16]    '-s0r='      '[-1 0 2]'
	+[ -31]    '-s0l1w'     '[-1 -1 4]' 	+[ -15]    '-n0c'       '[0 0 -4]'
	-[ -28]    '-s1l1c'     '[-2 -1 -4]'	-[ -14]    '-n0l1-'     '[0 -1 -1]'
	-[ -28]    '-s0l1-'     '[-1 -1 -1]'	-[ -13]    '-s2r1l='    '[-3 1 -2]'
	+[ -28]    '-n0l1c'     '[0 -1 -4]' 	+[ -13]    '-s0r1w'     '[-1 1 4]'
	-[ -28]    '-s2aw'      '[-3 0 8]'  	-[ -11]    '-s1r1l='    '[-2 1 -2]'
	-[ -25]    '-n0l='      '[0 0 -2]'  	-[ -11]    '-s0l1r='    '[-1 -1 2]'
	-[ -22]    '-s1r1+'     '[-2 1 1]'  	-[  -9]    '-s0l1c'     '[-1 -1 -4]'
	-[ -20]    '-s1c'       '[-2 0 -4]' 	-[  -9]    '-s0-'       '[-1 0 -1]'
	-[ -19]    '-s0h-'      '[-1 0 -9]' 	-[  -7]    '-s0r1r='    '[-1 1 2]'
	-[ -11]    '-s1h-'      '[-2 0 -9]' 	+[  -7]    '-n0l1c'     '[0 -1 -4]'
	-[  -4]    '-s1l1r>'    '[-2 -1 5]'

2014-10-16  Deniz Yuret  <dyuret@ku.edu.tr>

	* Best-points: (copied from vectorparser/ChangeLog)

	corpus: conllWSJToken (trn02-21:39832s,950028w,1820392m;dev22:1700s,40117w,76834m;tst23:2416s,56684w,108536m)
	arctype: archybrid (TODO: can archybrid13 be the same?)
	embedding: wikipedia2MUNK-100
	feats: fv021a
	kernel: rbf
	gamma: 0.333140404105682
	devscore: 0.0325506937033084 (nsv=96326, single=0.0328500403467215)
	test: (TODO)

	corpus: conll07EnglishToken (trn2:16588s,398439w,763702m;dev:1989s,48134w,92290m;trn=trn2+dev:18577s,446573w,855992m;tst:214s,5003w)
	arctype: archybrid
	embedding: wikipedia2MUNK-100 (TODO: other embeddings, bansal)
	feats: fv022b (TODO: compare with wsj feats)
	kernel: rbf
	gamma: 0.376747095368119
	tstscore: 0.0418668 (1-epoch full-trn/tst, nsv=57661)
	test: (TODO: debug run_static_dynamic)

2014-10-10  Deniz Yuret  <dyuret@ku.edu.tr>

	* conll07-dev: trn/dev/tst results (copied from vectorparser/ChangeLog)
	run_static_dynamic_1005_archybrid_conll07EnglishToken_wikipedia2MUNK_100_fv019_0_372063662109375_.out
	run/log2table.pl: extract table from log file:
	run/log2table.pl head_pct run_static_dynamic_1005_archybrid_conll07EnglishToken_wikipedia2MUNK_100_fv019_0_372063662109375_.out

	epoch	trn	dev	tst
	1	0.0644	0.1442	0.1455
	2	0.0408	0.1318	0.1403
	3	0.0211	0.1258	0.1317
	4	0.0148	0.1240	0.1305
	5	0.0108	0.1233	0.1269
	6	0.0087	0.1231	0.1267
	7	0.0073	0.1224	0.1235
	8	0.0065	0.1223	0.1235
	9	0.0063	0.1225	0.1235
	10	0.0060	0.1214	0.1219
	11	0.0060	0.1216	0.1219
	12	0.0056	0.1211	0.1201
	13	0.0056	0.1212	0.1219
	14	0.0055	0.1214	0.1221
	15	0.0053	0.1210	0.1217
	16	0.0052	0.1206	0.1229
	17	0.0050	0.1202	0.1241
	18	0.0050	0.1202	0.1243
	19	0.0048	0.1200	0.1239
	20	0.0049	0.1199	0.1251

2014-08-03  Deniz Yuret  <dyuret@ku.edu.tr>

	* conll07:

	- goldberg-nivre-2013: gives results for conll07 english using
	greedy parsers.  training is 02-11 (half regular PTB size), look
	at where the test data comes from (it is part of wsj23).

	- results on conll07.  check the source of the test corpus: paper
	claims subset of wsj23.  It seems to be wsj_2300 to part of
	wsj_2308.  Best UAS in conll07 paper: 90.63.  Goldberg-Nivre-2013
	best deterministic parser: 89.41 (Note: this is for easyfirst uas,
	best hybrid uas=8762, best eager uas=8869).  Both include
	punctuation (Correction: Only GN13 is including punct. C07 is
	probably using eval.pl and excluding punctuation, nothing
	specified in paper but it does say performance on conll07 task).

		 nsent	nword
	trnconll 18577	446573
	tstconll 214	5003

	Short	UAS+p	Paper
	Car07	90.63	X. Carreras. 2007. Experiments with a high-order projective dependency parser. In Proc. of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
	GN13	89.41	Yoav Goldberg and Joakim Nivre. 2013. Training Deterministic Parsers with Non-Deterministic Oracles.. TACL, vol 1, pp 403--414.

	* ptb:

	- results on PTB.  For beam parsers Zhang-Nivre-2011 test on PTB
	and report 92.9 on test (wsj23), 93.14 on dev (wsj22).  They also
	report two transition parsers: ZC08transition (91.4) HS10 (91.4),
	and four graph based parsers: MST (91.5), K08 (92.0), KC10 (93.0,
	92.9) These exclude punctuation: ZC08 says "Like McDonald et
	al. (2005), we evaluate the parsing accuracy by the precision of
	lexical heads (the percentage of input words, excluding
	punctuation, that have been assigned the correct parent) and by
	the percentage of complete matches, in which all words excluding
	punctuation have been assigned the correct parent."

		Secs	Sents 	Words
	Train 	2–21 	39832	950028
	Dev 	22 	1700	40117
	Test 	23 	2416	56684

	Short	UAS-p	Paper
	ZN11	92.9	Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Asso- ciation for Computational Linguistics: Human Language Technologies, pages 188–193.
	ZC08	92.1	Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 562–571.
	ZC08tr	91.4	just transition based features of ZC08.
	MST05	90.9	Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL, pages 91–98, Ann Arbor, Michigan, June.
	MST2	91.5	R McDonald and F Pereira. 2006. Online learning of approximate dependency parsing algorithms. In In Proc. of EACL, pages 81–88, Trento, Italy, April.
	MST1	90.7	First order result according to MP06.

	- conll07-ptb difference:

	from conll-xi/data/english/ptb/doc/README:
	The head and dependency relation fields were converted using the
	algorithms described in
	> Richard Johansson and Pierre Nugues (tentative title)
	> "Extended Constituent-to-Dependency Conversion for English"
	> (Submitted)
	> http://www.lucas.lth.se/lt/pennconverter
	This was run with the arguments: -conjAsHead -prepAsHead

	new website:
	http://nlp.cs.lth.se/software/treebank_converter
	http://fileadmin.cs.lth.se/nlp/software/pennconverter/pennconverter.jar

	The new arg -conll2007 is supposed to generate the conll07
	conventions.  But files are not identical.  It seems the old
	version of pennconverter (and thus the original conll07 data) had
	some conjunction bugs which were later fixed.  220/5003 (4.40%)
	head difference is the closest I can get to conll07.  On the
	training set about 5 sentences were deleted from ptb to conll07.
	Of the 446573 heads the closest I can get is 20298 difference
	(4.55%).  It seems if we are going to work with conll07 data we
	better use part of train as devel instead of trying to regenerate.
