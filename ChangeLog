2015-03-14  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/gparser.jl:
	- do not need to allocate p, y, etc. for the whole corpus in batch version.

	* TODO:
	+ return xy from gparser and bparser.
	- train. (need logploss, need earlystop?, try rmsprop?, adam?: adagrad may not do well on changing data)
	-- static oracle
	-- greedy parser
	-- beam parser
	- feature select.

	* INFO:
	+ biyofiz gpu memory is sufficient for nbeam*nbatch*ncpu <= 10000

	julia> @time b42y=KUparser.bparse(dev,net,feats,100,5,20);
	elapsed time: 261.986853539 seconds (3 MB allocated)
	julia> @time b41y=KUparser.bparse(dev,net,feats,10,50,20);
	elapsed time: 32.315438529 seconds (3 MB allocated)
	julia> find(b41y .!= b42y)
	1-element Array{Int64,1}:
	1324

	+ beam parser gives better head score with same model:

	julia> gold=map(x->x.head, dev);
	julia> mean(vcat(gold...) .== vcat(g0...))
	0.9105865343869183
	julia> mean(vcat(gold...) .== vcat(b41y...))
	0.9154473165989481

	* TODO: src/bparser.jl: minibatch version:
        - do not keep computing costs for finished sentences
        - do all beam elements finish simultaneously?
        - cache cost in parser?
	- repeated states on the beam?
	- nbeam=10 and nbeam=100 give the same parse?
	-- in all except sent 1324. beam1 vs beam10 differ in 224 sentences.

	* test/timing.jl: minibatch version speed test:

	# Greedy parser
	g0=KUparser.gparse(dev,gnet,feats)		elapsed time: 148.237107122 seconds
	g1=KUparser.gparse(dev,gnet,feats,1700)		elapsed time: 22.188889154 seconds
	g2=KUparser.gparse(dev,gnet,feats,1700,20)	elapsed time: 8.051600384 seconds
	g3=KUparser.gparse(dev,gnet,feats,100)		elapsed time: 26.318353452 seconds
	g4=KUparser.gparse(dev,gnet,feats,100,20)	elapsed time: 8.062792516 seconds

	# These are with beam size 1
	# Final parses same as g0
	b0=KUparser.bparse(dev,gnet,feats,1)		elapsed time: 156.000890758 seconds
	b1=KUparser.bparse(dev,gnet,feats,1,1700)	elapsed time: 30.145607698 seconds
	b2=KUparser.bparse(dev,gnet,feats,1,1700,20)	elapsed time: 8.18903106 seconds
	b3=KUparser.bparse(dev,gnet,feats,1,100)	elapsed time: 31.510566003 seconds
	b4=KUparser.bparse(dev,gnet,feats,1,100,20)	elapsed time: 8.234869093 seconds

	# These are with beam size 10 on the first 100 sentences of dev:
	# Final parses not same with g0[1:100]
	b01=KUparser.bparse(dev[1:100],gnet,feats,10)		elapsed time: 25.057099378 seconds
	b11=KUparser.bparse(dev[1:100],gnet,feats,10,100)	elapsed time: 14.705737159 seconds
	b21=KUparser.bparse(dev[1:100],gnet,feats,10,100,20)	elapsed time: 5.154158539 seconds
	b31=KUparser.bparse(dev[1:100],gnet,feats,10,10)	elapsed time: 16.469459534 seconds
	b41=KUparser.bparse(dev[1:100],gnet,feats,10,10,20)	elapsed time: 5.161457464 seconds

	# These are with beam size 100 on the first 100 sentences of dev:
	# Final parses same as b01
	b02=KUparser.bparse(dev[1:100],gnet,feats,100)		elapsed time: 137.201635533 seconds
	b12=KUparser.bparse(dev[1:100],gnet,feats,100,100)	elapsed time: 132.684041541 seconds
	b22=KUparser.bparse(dev[1:100],gnet,feats,100,100,20)	elapsed time: 18.501004294 seconds
	b32=KUparser.bparse(dev[1:100],gnet,feats,100,10)	elapsed time: 132.337806865 seconds
	b42=KUparser.bparse(dev[1:100],gnet,feats,100,10,20)	elapsed time: 18.514346591 seconds


2015-03-13  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/bparser.jl: speed test:

	# Time for greedy parser on 1700 sentence dev
	julia> @time g=KUparser.gparse(dev,gnet,feats);
	elapsed time: 149.497789522 seconds (4116 MB allocated, 1.06% gc time in 189 pauses with 0 full sweep)

	# Time for beam parser on same data with beam=1
	julia> @time b=KUparser.bparse(dev,gnet,feats,1);
	elapsed time: 159.778490775 seconds (7173 MB allocated, 3.76% gc time in 328 pauses with 9 full sweep)

	# Time on dev[1:100]
	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,1);
	elapsed time: 9.962360704 seconds (440 MB allocated, 5.42% gc time in 20 pauses with 1 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,5);
	elapsed time: 16.47657518 seconds (1940 MB allocated, 8.55% gc time in 89 pauses with 2 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,10);
	elapsed time: 27.863031912 seconds (3773 MB allocated, 12.66% gc time in 172 pauses with 6 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,50);
	elapsed time: 84.229235102 seconds (17997 MB allocated, 18.13% gc time in 806 pauses with 25 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,100);
	elapsed time: 158.230640377 seconds (35313 MB allocated, 16.89% gc time in 1549 pauses with 41 full sweep)

	# Time tests with ncpu=20

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,1,20);
	elapsed time: 9.237851461 seconds (2 MB allocated)
	elapsed time: 9.766501577 seconds (107 MB allocated, 4.21% gc time in 3 pauses with 1 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,5,20);
	elapsed time: 9.447347633 seconds (2 MB allocated)
	elapsed time: 9.975221305 seconds (107 MB allocated, 4.11% gc time in 3 pauses with 1 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,10,20);
	elapsed time: 13.658449233 seconds (2 MB allocated)
	elapsed time: 14.184126282 seconds (107 MB allocated, 2.89% gc time in 3 pauses with 1 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,50,20);
	elapsed time: 15.286421047 seconds (2 MB allocated)
	elapsed time: 15.809166647 seconds (107 MB allocated, 2.60% gc time in 3 pauses with 1 full sweep)

	julia> @time b=KUparser.bparse(dev[1:100],gnet,feats,100,20);
	elapsed time: 27.447839359 seconds (2 MB allocated)
	elapsed time: 28.380576273 seconds (107 MB allocated, 2.96% gc time in 5 pauses with 2 full sweep)


2015-03-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO: src/bparser.jl:
	- early stop?  could implement by copying into big x
	-- mincost may not be 0 for non-planar sentences
	- sortperm! suggest AbstractArray to julia
	- Now we sort the nc candidates and copy top np back to the beam
	- softperm! may not be necessary when nc <= beam (unless we care about the first candidate being the best)
	- do we need features when we have 0 or 1 valid move?
	- do not return y from gparser, return x,z consistently from each method
	- maybe have optional trn struct and return trn.x trn.y trn.nx
	- check for identical states on the beam.

2015-03-08  Deniz Yuret  <dyuret@ku.edu.tr>

	* Speed-test:

	julia@iui> @time KUparser.gparse(trn, net, feats, 2000, 20);
	elapsed time: 40.50693341 seconds (9502 MB allocated, 8.38% gc time in 14 pauses with 11 full sweep)
	elapsed time: 86.205662824 seconds (107430 MB allocated, 6.12% gc time in 86 pauses with 14 full sweep)

	julia@bio> @time KUparser.gparse(trn, net, feats, 500, 20);
	elapsed time: 75.61331844 seconds (9476 MB allocated, 3.19% gc time in 8 pauses with 6 full sweep)
	elapsed time: 112.552530706 seconds (107352 MB allocated, 4.21% gc time in 75 pauses with 9 full sweep)

	julia3@ilac> @time KUparser.gparse(trn, net, feats, 1700, 12);
	elapsed time: 86.231203 seconds (11419169336 bytes allocated, 7.88% gc time)
	elapsed time: 123.933778 seconds (75581738656 bytes allocated, 16.60% gc time)

2015-03-05  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- write bparser
	- can we use rnn's to learn parser-action sequences?
	- real use of rnn's is to map text to meaning!

	- should we try parallel for?
	- see how SharedArray is implemented and use it for net weights.
	
	* src/gparser.jl: gpu minibatch version:

	# dev has 1700 sentences and 40117 words
	# fastest matlab test speed was 26 sents/sec = 1.6298 ms/word
	# fastest matlab dump speed was 30 sents/sec = 1.4125 ms/word
	# fastest julia test speed is 80 sents/sec = 0.5286 ms/word
	# fastest julia dump speed is 116 sents/sec = 0.3652 ms/word

	# 3.6986 ms/word: parsing sentences one at a time
	include("fooparser.jl")
	julia> @time h2=KUparser.gparse(dev, n, Feats.fv021a);
	elapsed time: 148.37933571 seconds (4145 MB allocated, 0.83% gc time in 190 pauses with 0 full sweep)

	# 0.5286 ms/word: parsing sentences in minibatches
	julia> @time h1=KUparser.gparse(dev, n, Feats.fv021a, 1700);
	elapsed time: 21.206189373 seconds (6355 MB allocated, 8.16% gc time in 273 pauses with 1 full sweep)

	# 0.3652 ms/word: everything except KUnet prediction
	julia> @time h0=KUparser.gparse(dev, n, Feats.fv021a, false);
	elapsed time: 14.648858577 seconds (3488 MB allocated, 5.64% gc time in 160 pauses with 0 full sweep)

	# For training with Julia I expect 8 mins to parse, 2 mins to
	update, 10 mins per epoch.

2015-03-04  Deniz Yuret  <dyuret@ku.edu.tr>

	* src/features.jl:
	> @time foo=KUparser.gparse(dev[1:100], n, Feats.fv021a); # with rand!
	> elapsed time: 0.899239837 seconds (212 MB allocated, 4.95% gc time in 9 pauses with 0 full sweep)

	- Better but still allocating, why?

	> @time foo=KUparser.gparse(dev[1:100], n, Feats.fv021a); # with predict
	> elapsed time: 9.124658661 seconds (254 MB allocated, 0.79% gc time in 11 pauses with 0 full sweep)

	* features.jl: get rid of avec allocation.
	* net.jl: find what allocates 6kb?

2015-03-03  Deniz Yuret  <dyuret@ku.edu.tr>

	* TODO:
	- get rid of allocation
	- optimize feature extraction
	- cuda parser?
	- solve pmap problem (shared arrays, pointers?)

	* timing: dev is the 1700 sentences:
	sent_pct: 0.5888
	head_pct: 0.0894
	word_pct: 0.0785
	move_pct: 0.0340

	We are doing about 11 sentences/second: (90ms/sent)

	julia> @time foo=KUparser.gparse(dev[1:10], net, Fmats.fv021a)
	elapsed time: 1.078577283 seconds (65 MB allocated, 0.85% gc time in 3 pauses with 0 full sweep)
	julia> @time foo=KUparser.gparse(dev[1:100], net, Fmats.fv021a);
	elapsed time: 9.161665507 seconds (557 MB allocated, 0.80% gc time in 25 pauses with 0 full sweep)
	julia> @time foo=KUparser.gparse(dev, net, Fmats.fv021a);
	elapsed time: 152.361663243 seconds (9084 MB allocated, 1.26% gc time in 415 pauses with 0 full sweep)

	The feature construction is actually pretty fast: (10ms/sent)

	After replacing "predict" with "rand":
	julia> @time foo=KUparser.gparse(dev[1:100], net, Fmats.fv021a);
	elapsed time: 1.007098236 seconds (510 MB allocated, 11.25% gc time in 23 pauses with 0 full sweep)

	However there is too much allocation and speed can be improved.

	More impact would be to batch sentences first.
